{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUe1ALpl/v9StAj02VAkEM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ETL FIDC v1.0 - CÃ“DIGO COMPLETO\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# Copie e cole cada bloco em uma cÃ©lula separada no Google Colab\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 1: IMPORTS\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","import pandas as pd\n","import requests\n","import base64\n","import xml.etree.ElementTree as ET\n","import time\n","import os\n","from typing import Dict, List\n","from tqdm import tqdm\n","\n","print(\"âœ… Bibliotecas importadas!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmofAZXATagu","executionInfo":{"status":"ok","timestamp":1765813710084,"user_tz":180,"elapsed":25,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"0654b4b4-10fb-4dbc-dcdb-ee1a298bf0c1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Bibliotecas importadas!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 2: CONFIGURAÃ‡Ã•ES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","URL_API_BUSCA = \"https://fnet.bmfbovespa.com.br/fnet/publico/pesquisarGerenciadorDocumentosDados\"\n","URL_API_DOWNLOAD = \"https://fnet.bmfbovespa.com.br/fnet/publico/downloadDocumento\"\n","\n","HEADERS = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n","}\n","\n","CONFIG = {\n","    'delay': 2,\n","    'timeout_busca': 10,\n","    'timeout_download': 20,\n","    'limite_docs': 200\n","}\n","\n","print(\"âœ… ConfiguraÃ§Ãµes definidas!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8J0i1wrTchq","executionInfo":{"status":"ok","timestamp":1765813711781,"user_tz":180,"elapsed":30,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"a9ddc155-8c3b-4309-a9a4-61d03c73446e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ConfiguraÃ§Ãµes definidas!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 3: FUNÃ‡Ã•ES AUXILIARES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def converter_valor(texto: str) -> float:\n","    if not texto or not str(texto).strip():\n","        return 0.0\n","    try:\n","        texto_limpo = str(texto).replace('.', '').replace(',', '.')\n","        return float(texto_limpo)\n","    except:\n","        return 0.0\n","\n","\n","def buscar_valor_xml(root: ET.Element, caminho: str):\n","    elemento = root.find(f'.//{caminho}')\n","    if elemento is not None and elemento.text:\n","        try:\n","            return converter_valor(elemento.text)\n","        except:\n","            return elemento.text.strip()\n","    return 0.0 if '/' in caminho else ''\n","\n","\n","print(\"âœ… FunÃ§Ãµes auxiliares criadas!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylTTipz0Tkr1","executionInfo":{"status":"ok","timestamp":1765813713483,"user_tz":180,"elapsed":25,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"38868420-2cb2-4536-d542-65ae3580044b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FunÃ§Ãµes auxiliares criadas!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 4: EXTRAÃ‡ÃƒO XML (VERSÃƒO CORRIGIDA v1.0.2 FINAL)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def extrair_dados_xml(xml_content: bytes, cnpj_fallback: str = '') -> Dict:\n","    \"\"\"\n","    Extrai TODOS os campos relevantes do XML de Informe Mensal FIDC.\n","\n","    CORREÃ‡Ã•ES v1.0.2:\n","    - NPL calculado CORRETAMENTE como percentual (decimal)\n","    - Carteira total considera ambos os tipos de crÃ©dito\n","    - ValidaÃ§Ã£o adicional para evitar valores absurdos\n","\n","    Args:\n","        xml_content: ConteÃºdo XML em bytes\n","        cnpj_fallback: CNPJ para usar se nÃ£o encontrado no XML\n","\n","    Returns:\n","        Dict com 90+ campos estruturados\n","    \"\"\"\n","\n","    try:\n","        root = ET.fromstring(xml_content)\n","    except Exception as e:\n","        print(f\"âŒ Erro ao parsear XML: {e}\")\n","        return {}\n","\n","    dados = {\n","        # â•â•â• IDENTIFICAÃ‡ÃƒO â•â•â•\n","        'CNPJ_FUNDO': buscar_valor_xml(root, 'NR_CNPJ_FUNDO') or cnpj_fallback,\n","        'DATA_COMPETENCIA': buscar_valor_xml(root, 'DT_COMPT'),\n","\n","        # â•â•â• ATIVOS PRINCIPAIS â•â•â•\n","        'ATIVO_TOTAL': buscar_valor_xml(root, 'VL_SOM_APLIC_ATIVO'),\n","        'DISPONIBILIDADES': buscar_valor_xml(root, 'VL_DISPONIB'),\n","        'CARTEIRA_TOTAL': buscar_valor_xml(root, 'VL_CARTEIRA'),\n","\n","        # â•â•â• CRÃ‰DITOS EXISTENTES (COM AQUISIÃ‡ÃƒO SUBSTANCIAL) â•â•â•\n","        'CREDITOS_ADQUIRIDOS': buscar_valor_xml(root, 'CRED_EXISTE/VL_SOM_DICRED_AQUIS'),\n","        'CRED_VENCIDOS_ADIMPLENTES': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_VENC_ADIMPL'),\n","        'CRED_VENCIDOS_INADIMPLENTES': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_VENC_INAD'),\n","        'CRED_INADIMPLENCIA': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_INAD'),\n","\n","        # â•â•â• DIREITOS CREDITÃ“RIOS (SEM AQUISIÃ‡ÃƒO SUBSTANCIAL) â•â•â•\n","        'DICRED_TOTAL': buscar_valor_xml(root, 'DICRED/VL_DICRED'),\n","        'DICRED_INADIMPLENCIA': buscar_valor_xml(root, 'DICRED/VL_DICRED_EXISTE_INAD'),\n","\n","        # â•â•â• SEGMENTAÃ‡ÃƒO DA CARTEIRA â•â•â•\n","        'SEGMT_INDUSTRIAL': buscar_valor_xml(root, 'CART_SEGMT/VL_IND'),\n","        'SEGMT_AGRONEGOCIO': buscar_valor_xml(root, 'CART_SEGMT/VL_AGRONEG'),\n","        'SEGMT_ACAO_JUDICIAL': buscar_valor_xml(root, 'CART_SEGMT/VL_ACAO_JUDIC'),\n","\n","        # â•â•â• SEGMENTO FINANCEIRO â•â•â•\n","        'SEGMT_FINANC_CORPORATIVO': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_CRED_CORPOR'),\n","\n","        # â•â•â• SETOR PÃšBLICO â•â•â•\n","        'SEGMT_PRECATORIOS': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SETOR_PUBLIC_PRECAT'),\n","    }\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # CÃLCULO DE INDICADORES (âœ… VERSÃƒO FINAL CORRIGIDA)\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    # 1. INADIMPLÃŠNCIA CONSOLIDADA (SOMA, nÃ£o MAX!)\n","    inadimpl_cred = float(dados.get('CRED_INADIMPLENCIA', 0) or 0)\n","    inadimpl_dicred = float(dados.get('DICRED_INADIMPLENCIA', 0) or 0)\n","    dados['INADIMPLENCIA_TOTAL'] = inadimpl_cred + inadimpl_dicred\n","\n","    # 2. CARTEIRA TOTAL CONSOLIDADA (ambos os tipos)\n","    creditos_com_aquisicao = float(dados.get('CREDITOS_ADQUIRIDOS', 0) or 0)\n","    creditos_sem_aquisicao = float(dados.get('DICRED_TOTAL', 0) or 0)\n","    carteira_total_bruta = creditos_com_aquisicao + creditos_sem_aquisicao\n","\n","    # 3. ATIVO E DISPONIBILIDADES\n","    ativo_total = float(dados.get('ATIVO_TOTAL', 0) or 0)\n","    disponibilidades = float(dados.get('DISPONIBILIDADES', 0) or 0)\n","    carteira_liquida = float(dados.get('CARTEIRA_TOTAL', 0) or 0)\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # CÃLCULO DOS INDICADORES EM FORMATO DECIMAL (0.52 = 52%)\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    # âœ… Ãndice NPL (Non-Performing Loans) - DECIMAL\n","    # Se carteira bruta > 0, calcula NPL\n","    # Se carteira bruta = 0 mas tem inadimplÃªncia, NPL = 100%\n","    # Caso contrÃ¡rio, NPL = 0%\n","    if carteira_total_bruta > 0:\n","        npl_calculado = dados['INADIMPLENCIA_TOTAL'] / carteira_total_bruta\n","        # ğŸ”’ VALIDAÃ‡ÃƒO: NPL nÃ£o pode ser > 100% (1.0)\n","        dados['INDICE_NPL_DECIMAL'] = min(npl_calculado, 1.0)\n","    elif dados['INADIMPLENCIA_TOTAL'] > 0:\n","        # Se tem inadimplÃªncia mas carteira = 0, assume 100% NPL\n","        dados['INDICE_NPL_DECIMAL'] = 1.0\n","    else:\n","        dados['INDICE_NPL_DECIMAL'] = 0.0\n","\n","    # âœ… Taxa de liquidez - DECIMAL\n","    if ativo_total > 0:\n","        dados['TAXA_LIQUIDEZ_DECIMAL'] = disponibilidades / ativo_total\n","    else:\n","        dados['TAXA_LIQUIDEZ_DECIMAL'] = 0.0\n","\n","    # âœ… ConcentraÃ§Ã£o em crÃ©dito - DECIMAL\n","    # Usa a carteira LÃQUIDA (apÃ³s provisÃµes) que estÃ¡ em CARTEIRA_TOTAL\n","    if ativo_total > 0:\n","        dados['CONCENTRACAO_CREDITO_DECIMAL'] = carteira_liquida / ativo_total\n","    else:\n","        dados['CONCENTRACAO_CREDITO_DECIMAL'] = 0.0\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # CAMPOS ADICIONAIS PARA AUDITORIA\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    # Salvar carteira bruta para conferÃªncia\n","    dados['CARTEIRA_BRUTA'] = carteira_total_bruta\n","\n","    # ProvisÃµes (diferenÃ§a entre bruta e lÃ­quida)\n","    if carteira_total_bruta > 0 and carteira_liquida > 0:\n","        dados['PROVISOES_TOTAL'] = carteira_total_bruta - carteira_liquida\n","    else:\n","        dados['PROVISOES_TOTAL'] = 0.0\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # VALIDAÃ‡Ã•ES FINAIS\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    # ğŸ”’ Garantir que INDICE_NPL_DECIMAL estÃ¡ entre 0 e 1\n","    if dados['INDICE_NPL_DECIMAL'] < 0:\n","        dados['INDICE_NPL_DECIMAL'] = 0.0\n","    elif dados['INDICE_NPL_DECIMAL'] > 1:\n","        dados['INDICE_NPL_DECIMAL'] = 1.0\n","\n","    # ğŸ”’ Log de alerta se valores suspeitos\n","    if dados['INDICE_NPL_DECIMAL'] > 10:  # Se passou 10 (1000%), tem erro!\n","        print(f\"âš ï¸ ALERTA: CNPJ {dados['CNPJ_FUNDO']} com NPL suspeito: {dados['INDICE_NPL_DECIMAL']}\")\n","        print(f\"   Inadimpl: {dados['INADIMPLENCIA_TOTAL']:,.2f}\")\n","        print(f\"   Carteira: {carteira_total_bruta:,.2f}\")\n","\n","    return dados\n","\n","\n","print(\"âœ… FunÃ§Ã£o de extraÃ§Ã£o v1.0.2 FINAL criada!\")\n","print(\"\")\n","print(\"ğŸ“Š CorreÃ§Ãµes v1.0.2:\")\n","print(\"   âœ… NPL calculado como DECIMAL (0.52 = 52%)\")\n","print(\"   âœ… ValidaÃ§Ã£o: NPL entre 0% e 100%\")\n","print(\"   âœ… Tratamento de carteira vazia\")\n","print(\"   âœ… ConversÃ£o explÃ­cita para float\")\n","print(\"   âœ… Logs de alerta para valores suspeitos\")\n","print(\"\")\n","print(\"ğŸ” Campos de auditoria:\")\n","print(\"   â€¢ CARTEIRA_BRUTA (crÃ©ditos totais antes de provisÃµes)\")\n","print(\"   â€¢ PROVISOES_TOTAL (diferenÃ§a bruta - lÃ­quida)\")\n","print(\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzFaHGQiTtu0","executionInfo":{"status":"ok","timestamp":1765808183448,"user_tz":180,"elapsed":30,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"7744fdeb-5ecc-4f65-a0d1-64a279158cee"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FunÃ§Ã£o de extraÃ§Ã£o v1.0.1 criada!\n","\n","ğŸ“Š CorreÃ§Ãµes aplicadas:\n","   â€¢ NPL: Soma COM + SEM aquisiÃ§Ã£o\n","   â€¢ Carteira: Considera ambos os tipos\n","   â€¢ Formato: DECIMAL (0.52 = 52%)\n","\n","ğŸ” Campos adicionais:\n","   â€¢ CARTEIRA_BRUTA (para auditoria)\n","   â€¢ PROVISOES_TOTAL (carteira bruta - lÃ­quida)\n","\n","âš ï¸ ATENÃ‡ÃƒO:\n","   No Power BI, formate as colunas _DECIMAL como Percentual!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 5: ETL PRINCIPAL\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def processar_cnpj(cnpj: str) -> List[Dict]:\n","    resultados = []\n","\n","    try:\n","        # Buscar documentos\n","        params = {\n","            'd': 0,\n","            's': 0,\n","            'l': CONFIG['limite_docs'],\n","            'cnpjFundo': cnpj\n","        }\n","\n","        resp = requests.get(\n","            URL_API_BUSCA,\n","            params=params,\n","            headers=HEADERS,\n","            timeout=CONFIG['timeout_busca']\n","        )\n","        resp.raise_for_status()\n","\n","        data = resp.json().get('data', [])\n","        if not data:\n","            return []\n","\n","        df_docs = pd.DataFrame(data)\n","\n","        # Filtrar informe mensal\n","        df_mensal = df_docs[\n","            (df_docs['tipoDocumento'].str.strip() == \"Informe Mensal Estruturado\") &\n","            (df_docs['situacaoDocumento'].str.strip() == \"A\")\n","        ].copy()\n","\n","        if df_mensal.empty:\n","            return []\n","\n","        # Selecionar mais recente\n","        df_mensal['dataRef'] = pd.to_datetime(\n","            df_mensal['dataReferencia'].str[3:] + '-' +\n","            df_mensal['dataReferencia'].str[:2] + '-01',\n","            format='%Y-%m-%d',\n","            errors='coerce'\n","        )\n","\n","        df_mensal = df_mensal.dropna(subset=['dataRef'])\n","        if df_mensal.empty:\n","            return []\n","\n","        df_mensal = df_mensal.sort_values('dataRef', ascending=False)\n","        doc = df_mensal.iloc[0]\n","\n","        # Baixar XML\n","        url_download = f\"{URL_API_DOWNLOAD}?id={doc['id']}\"\n","        resp_xml = requests.get(\n","            url_download,\n","            headers=HEADERS,\n","            timeout=CONFIG['timeout_download']\n","        )\n","        resp_xml.raise_for_status()\n","\n","        xml_content = base64.b64decode(resp_xml.content)\n","\n","        # Extrair dados\n","        dados = extrair_dados_xml(xml_content, cnpj)\n","        dados['STATUS'] = 'SUCESSO'\n","        dados['DATA_REFERENCIA_DOC'] = doc.get('dataReferencia', '')\n","        dados['TIPO_COLETA'] = 'ULTIMO_MES'\n","\n","        resultados.append(dados)\n","\n","    except Exception as e:\n","        resultados.append({\n","            'CNPJ_FUNDO': cnpj,\n","            'STATUS': 'ERRO',\n","            'MENSAGEM_ERRO': str(e)\n","        })\n","\n","    return resultados\n","\n","\n","print(\"âœ… Pipeline ETL criado!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPJmQ2TKTz4h","executionInfo":{"status":"ok","timestamp":1765813721203,"user_tz":180,"elapsed":50,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"b943dcda-7b26-4fe6-e88b-56a32d795af3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Pipeline ETL criado!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 6: UPLOAD DE DADOS (CORRIGIDO)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","from google.colab import files\n","import io\n","\n","print(\"ğŸ“¤ FaÃ§a upload do arquivo: lista_cnpjs_fidc.csv\")\n","uploaded = files.upload()\n","\n","filename = list(uploaded.keys())[0]\n","\n","# âœ… CORREÃ‡ÃƒO: Detectar automaticamente o separador\n","# Tentar primeiro com vÃ­rgula, depois ponto e vÃ­rgula\n","try:\n","    df_cnpjs = pd.read_csv(io.BytesIO(uploaded[filename]), sep=',')\n","    # Verificar se carregou corretamente\n","    if 'CNPJ' not in df_cnpjs.columns and len(df_cnpjs.columns) == 1:\n","        # Se sÃ³ tem 1 coluna, o separador estÃ¡ errado\n","        raise ValueError(\"Separador incorreto\")\n","except:\n","    # Tentar com ponto e vÃ­rgula\n","    df_cnpjs = pd.read_csv(io.BytesIO(uploaded[filename]), sep=';')\n","\n","print(f\"\\nâœ… Arquivo carregado: {filename}\")\n","print(f\"   â€¢ Total de CNPJs: {len(df_cnpjs):,}\")\n","print(f\"   â€¢ Colunas encontradas: {list(df_cnpjs.columns)}\")\n","\n","# Verificar se tem coluna CNPJ\n","if 'CNPJ' not in df_cnpjs.columns:\n","    print(\"\\nâš ï¸ ATENÃ‡ÃƒO: Coluna 'CNPJ' nÃ£o encontrada!\")\n","    print(f\"   Colunas disponÃ­veis: {list(df_cnpjs.columns)}\")\n","    print(\"\\n   Por favor, verifique se o CSV tem a coluna 'CNPJ'\")\n","else:\n","    print(f\"\\nğŸ“Š Primeiras 5 linhas:\")\n","    print(df_cnpjs.head())\n","    print(f\"\\nâœ… Pronto para processar!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"7CcxV94FUwBs","executionInfo":{"status":"ok","timestamp":1765813732847,"user_tz":180,"elapsed":8137,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"945a0a1e-6176-4d66-a573-516c522d06a4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¤ FaÃ§a upload do arquivo: lista_cnpjs_fidc.csv\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1cc1fa5b-f5e2-4428-937d-68cd031a2dfc\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1cc1fa5b-f5e2-4428-937d-68cd031a2dfc\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving lista_cnpjs_fidc.csv to lista_cnpjs_fidc (1).csv\n","\n","âœ… Arquivo carregado: lista_cnpjs_fidc (1).csv\n","   â€¢ Total de CNPJs: 441\n","   â€¢ Colunas encontradas: ['CNPJ', 'NOME_FUNDO', 'CNPJ_ORIGINAL']\n","\n","ğŸ“Š Primeiras 5 linhas:\n","             CNPJ                                         NOME_FUNDO  \\\n","0  51199121000145  2MONEY RESP LIMITADA FIDC NP SUBORDINADA JÃšNIOR 1   \n","1  47388724000118                   3R RESP LIMITADA FIDC NP ÃšNICA 1   \n","2  52362629000185                                    3T FICFIDC NP 1   \n","3  50356663000111                        ABF VISION FIDC NP SÃŠNIOR 1   \n","4  41609997000169    ACERTO GRUPO INTER FIDC NP SUBORDINADA JÃšNIOR 1   \n","\n","        CNPJ_ORIGINAL  \n","0  51.199.121/0001-45  \n","1  47.388.724/0001-18  \n","2  52.362.629/0001-85  \n","3  50.356.663/0001-11  \n","4  41.609.997/0001-69  \n","\n","âœ… Pronto para processar!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 7: PROCESSAMENTO\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","lista_cnpjs = df_cnpjs['CNPJ'].astype(str).str.zfill(14).tolist()\n","\n","print(f\"ğŸš€ Processando {len(lista_cnpjs)} CNPJs...\")\n","print(f\"â±ï¸ Tempo estimado: ~{len(lista_cnpjs) * CONFIG['delay'] / 60:.1f} minutos\\n\")\n","\n","todos_resultados = []\n","\n","for cnpj in tqdm(lista_cnpjs, desc=\"Progresso\"):\n","    resultados = processar_cnpj(cnpj)\n","    todos_resultados.extend(resultados)\n","    time.sleep(CONFIG['delay'])\n","\n","df_resultado = pd.DataFrame(todos_resultados)\n","\n","print(f\"\\nâœ… ConcluÃ­do!\")\n","print(f\"   Total: {len(df_resultado):,}\")\n","print(f\"   Sucesso: {(df_resultado['STATUS'] == 'SUCESSO').sum():,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzWeUygUVCwf","executionInfo":{"status":"ok","timestamp":1765815006229,"user_tz":180,"elapsed":1270717,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"07d04495-5d02-440f-992a-d712eeb66b0e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Processando 441 CNPJs...\n","â±ï¸ Tempo estimado: ~14.7 minutos\n","\n"]},{"output_type":"stream","name":"stderr","text":["Progresso: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 441/441 [21:10<00:00,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… ConcluÃ­do!\n","   Total: 441\n","   Sucesso: 441\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CÃ‰LULA 8: EXPORTAÃ‡ÃƒO (SEM NOTAÃ‡ÃƒO CIENTÃFICA!)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"ğŸ’¾ Exportando arquivos...\\n\")\n","\n","# Arquivo completo\n","arquivo = 'fidc_dados_v1.csv'\n","\n","df_resultado.to_csv(\n","    arquivo,\n","    index=False,\n","    encoding='utf-8-sig',\n","    sep=';',\n","    decimal=',',\n","    float_format='%.2f'  # â† CRÃTICO: Remove notaÃ§Ã£o cientÃ­fica!\n",")\n","\n","print(f\"âœ… {arquivo} salvo ({len(df_resultado):,} registros)\")\n","\n","# Download\n","files.download(arquivo)\n","\n","print(\"\\nğŸ‰ PROCESSO CONCLUÃDO!\")\n","print(\"\\nğŸ“‹ PrÃ³ximos passos:\")\n","print(\"   1. Importe o CSV no Power BI\")\n","print(\"   2. Verifique que nÃ£o hÃ¡ notaÃ§Ã£o cientÃ­fica\")\n","print(\"   3. Configure tipos de dados se necessÃ¡rio\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"mMJiYy4AelQj","executionInfo":{"status":"ok","timestamp":1765816052823,"user_tz":180,"elapsed":50,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"9e5cedd7-7614-4eb3-fef3-d526d6b4e6c6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ’¾ Exportando arquivos...\n","\n","âœ… fidc_dados_v1.csv salvo (441 registros)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2cb7d5aa-a4ea-484d-b191-bf971d795a8a\", \"fidc_dados_v1.csv\", 89299)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ğŸ‰ PROCESSO CONCLUÃDO!\n","\n","ğŸ“‹ PrÃ³ximos passos:\n","   1. Importe o CSV no Power BI\n","   2. Verifique que nÃ£o hÃ¡ notaÃ§Ã£o cientÃ­fica\n","   3. Configure tipos de dados se necessÃ¡rio\n"]}]},{"cell_type":"markdown","metadata":{"id":"d1b24024"},"source":["# Task\n","Perform data cleaning and validation on the `fidc_dados_v1.csv` file by loading it, cleaning the 'CNPJ_FUNDO' column, converting relevant numeric and date columns, calculating and validating 'Liquidez' and 'NPL', calculating 'CARTEIRA_LIQUIDA_CALC' and 'SEM_POSICAO_FLAG', then generate a QA issues report as `qa_issues.csv` and export the final cleaned snapshot as `cleaned_snapshot.csv`."]},{"cell_type":"markdown","metadata":{"id":"a5e34482"},"source":["## Load and Initial Data Cleaning\n","\n","### Subtask:\n","Load the `fidc_dados_v1.csv` file, ensuring correct parsing with `sep=';'` and `decimal=','`. Implement a robust function to clean the 'CNPJ_FUNDO' column, removing any non-numeric characters and ensuring it's a 14-digit string. Initialize flag columns as False.\n"]},{"cell_type":"markdown","metadata":{"id":"9fe20524"},"source":["## Load and Initial Data Cleaning\n","\n","This section focuses on loading the extracted FIDC data and performing initial cleaning and preparation. We will load the `fidc_dados_v1.csv` file, ensuring correct parsing for numeric values and separators. A key step involves standardizing the 'CNPJ_FUNDO' column by removing non-numeric characters and ensuring a consistent 14-digit format. Finally, we will initialize several flag columns, all set to `False`, which will be used in subsequent data validation steps."]},{"cell_type":"markdown","metadata":{"id":"d7ca69bc"},"source":["**Reasoning**:\n","Now I will load the `fidc_dados_v1.csv` file, clean the 'CNPJ_FUNDO' column using a robust function to ensure a consistent 14-digit format, and initialize the specified flag columns with `False` values as requested in the subtask.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"e4ec62b2","executionInfo":{"status":"error","timestamp":1765818150500,"user_tz":180,"elapsed":89,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"47fd0a63-3ae1-416b-bf44-e67e0167e125"},"source":["df_cleaned = pd.read_csv(\n","    'fidc_dados_v1.csv',\n","    sep=';',\n","    decimal=','\n",")\n","\n","def clean_cnpj(cnpj_str: str) -> str:\n","    \"\"\"\n","    Cleans a CNPJ string by removing non-digit characters and padding with leading zeros\n","    to ensure it is a 14-digit string.\n","    \"\"\"\n","    if not isinstance(cnpj_str, str):\n","        cnpj_str = str(cnpj_str)\n","\n","    # Remove all non-digit characters\n","    cleaned = ''.join(filter(str.isdigit, cnpj_str))\n","\n","    # Pad with leading zeros if shorter than 14 characters\n","    return cleaned.zfill(14)\n","\n","# Apply the cleaning function to the 'CNPJ_FUNDO' column\n","df_cleaned['CNPJ_FUNDO'] = df_cleaned['CNPJ_FUNDO'].apply(clean_cnpj)\n","\n","# Initialize flag columns with False\n","flag_columns = [\n","    'ATIVO_ZERO_FLAG',\n","    'DIVERGE_LIQ_FLAG',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG',\n","    'DIVERGE_NPL_FLAG',\n","    'SEM_POSICAO_FLAG'\n","]\n","\n","for col in flag_columns:\n","    df_cleaned[col] = False\n","\n","print(\"âœ… fidc_dados_v1.csv loaded, 'CNPJ_FUNDO' cleaned, and flag columns initialized!\")\n","print(f\"   Shape of df_cleaned: {df_cleaned.shape}\")\n","print(f\"   First 5 rows of df_cleaned:\\n{df_cleaned.head()}\")"],"execution_count":16,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'fidc_dados_v1.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3659127538.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_cleaned = pd.read_csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'fidc_dados_v1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fidc_dados_v1.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"208040b3"},"source":["**Reasoning**:\n","The previous code failed because `fidc_dados_v1.csv` was not found. Although `to_csv` was called, the file might not be accessible directly from the working directory, or there might be an issue with how the filename was used. The kernel state confirms `fidc_dados_v1.csv` exists in `/content/`. Also, to avoid potential data type issues where CNPJ might be read as a float and lose precision (as indicated by the `df_resultado` variable in the kernel state, where 'CNPJ_FUNDO' is a float in scientific notation), I will explicitly load 'CNPJ_FUNDO' as a string and adjust the `clean_cnpj` function to handle such string representations (e.g., '12345678901234.00') to correctly extract the 14-digit CNPJ.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"8841a276","executionInfo":{"status":"error","timestamp":1765818180981,"user_tz":180,"elapsed":62,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"b24a9ed4-6104-42a6-b21c-e2f021c68eb7"},"source":["df_cleaned = pd.read_csv(\n","    '/content/fidc_dados_v1.csv', # Use absolute path\n","    sep=';',\n","    decimal=',',\n","    dtype={'CNPJ_FUNDO': str} # Ensure CNPJ is read as string to preserve full value\n",")\n","\n","def clean_cnpj(cnpj_str: str) -> str:\n","    \"\"\"\n","    Cleans a CNPJ string by removing non-digit characters and ensuring it is a 14-digit string.\n","    It handles cases where the CNPJ might be stored as a float string (e.g., '12345678901234.00').\n","    \"\"\"\n","    if not isinstance(cnpj_str, str):\n","        cnpj_str = str(cnpj_str)\n","\n","    # Remove all non-digit characters\n","    cleaned = ''.join(filter(str.isdigit, cnpj_str))\n","\n","    # Ensure it's 14 digits, taking the first 14 if longer (to remove '.00' suffix if present)\n","    # and padding with leading zeros if shorter.\n","    return cleaned[:14].zfill(14)\n","\n","# Apply the cleaning function to the 'CNPJ_FUNDO' column\n","df_cleaned['CNPJ_FUNDO'] = df_cleaned['CNPJ_FUNDO'].apply(clean_cnpj)\n","\n","# Initialize flag columns with False\n","flag_columns = [\n","    'ATIVO_ZERO_FLAG',\n","    'DIVERGE_LIQ_FLAG',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG',\n","    'DIVERGE_NPL_FLAG',\n","    'SEM_POSICAO_FLAG'\n","]\n","\n","for col in flag_columns:\n","    df_cleaned[col] = False\n","\n","print(\"âœ… fidc_dados_v1.csv loaded, 'CNPJ_FUNDO' cleaned, and flag columns initialized!\")\n","print(f\"   Shape of df_cleaned: {df_cleaned.shape}\")\n","print(f\"   First 5 rows of df_cleaned:\\n{df_cleaned.head()}\")"],"execution_count":17,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/fidc_dados_v1.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3589592885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_cleaned = pd.read_csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'/content/fidc_dados_v1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use absolute path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'CNPJ_FUNDO'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# Ensure CNPJ is read as string to preserve full value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fidc_dados_v1.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"8e1ba63c"},"source":["**Reasoning**:\n","The previous code failed because the file path `/content/fidc_dados_v1.csv` was incorrect. The file `fidc_dados_v1.csv` was saved in the current working directory, as indicated by the `arquivo` variable and the previous `to_csv` output. I will correct the file path to `fidc_dados_v1.csv`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"b7ee3a9f","executionInfo":{"status":"error","timestamp":1765818191044,"user_tz":180,"elapsed":77,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"88216850-fdc1-4446-eb9b-f03df625012c"},"source":["df_cleaned = pd.read_csv(\n","    'fidc_dados_v1.csv', # Corrected path to current directory\n","    sep=';',\n","    decimal=',',\n","    dtype={'CNPJ_FUNDO': str} # Ensure CNPJ is read as string to preserve full value\n",")\n","\n","def clean_cnpj(cnpj_str: str) -> str:\n","    \"\"\"\n","    Cleans a CNPJ string by removing non-digit characters and ensuring it is a 14-digit string.\n","    It handles cases where the CNPJ might be stored as a float string (e.g., '12345678901234.00').\n","    \"\"\"\n","    if not isinstance(cnpj_str, str):\n","        cnpj_str = str(cnpj_str)\n","\n","    # Remove all non-digit characters\n","    cleaned = ''.join(filter(str.isdigit, cnpj_str))\n","\n","    # Ensure it's 14 digits, taking the first 14 if longer (to remove '.00' suffix if present)\n","    # and padding with leading zeros if shorter.\n","    return cleaned[:14].zfill(14)\n","\n","# Apply the cleaning function to the 'CNPJ_FUNDO' column\n","df_cleaned['CNPJ_FUNDO'] = df_cleaned['CNPJ_FUNDO'].apply(clean_cnpj)\n","\n","# Initialize flag columns with False\n","flag_columns = [\n","    'ATIVO_ZERO_FLAG',\n","    'DIVERGE_LIQ_FLAG',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG',\n","    'DIVERGE_NPL_FLAG',\n","    'SEM_POSICAO_FLAG'\n","]\n","\n","for col in flag_columns:\n","    df_cleaned[col] = False\n","\n","print(\"âœ… fidc_dados_v1.csv loaded, 'CNPJ_FUNDO' cleaned, and flag columns initialized!\")\n","print(f\"   Shape of df_cleaned: {df_cleaned.shape}\")\n","print(f\"   First 5 rows of df_cleaned:\\n{df_cleaned.head()}\")"],"execution_count":18,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'fidc_dados_v1.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-724524449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_cleaned = pd.read_csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'fidc_dados_v1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Corrected path to current directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'CNPJ_FUNDO'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# Ensure CNPJ is read as string to preserve full value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fidc_dados_v1.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"878b9c50"},"source":["**Reasoning**:\n","The previous code failed with `FileNotFoundError` because the `fidc_dados_v1.csv` file was not found, as it was likely saved with a different name like `fidc_dados_v1 (2).csv`. I will correct the file path to `/content/fidc_dados_v1 (2).csv` to load the existing file.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bded496","executionInfo":{"status":"ok","timestamp":1765818221502,"user_tz":180,"elapsed":95,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"33a5cced-95e7-483d-8d47-b453c26f2de2"},"source":["df_cleaned = pd.read_csv(\n","    '/content/fidc_dados_v1 (2).csv', # Corrected path to the existing file\n","    sep=';',\n","    decimal=',',\n","    dtype={'CNPJ_FUNDO': str} # Ensure CNPJ is read as string to preserve full value\n",")\n","\n","def clean_cnpj(cnpj_str: str) -> str:\n","    \"\"\"\n","    Cleans a CNPJ string by removing non-digit characters and ensuring it is a 14-digit string.\n","    It handles cases where the CNPJ might be stored as a float string (e.g., '12345678901234.00').\n","    \"\"\"\n","    if not isinstance(cnpj_str, str):\n","        cnpj_str = str(cnpj_str)\n","\n","    # Remove all non-digit characters\n","    cleaned = ''.join(filter(str.isdigit, cnpj_str))\n","\n","    # Ensure it's 14 digits, taking the first 14 if longer (to remove '.00' suffix if present)\n","    # and padding with leading zeros if shorter.\n","    return cleaned[:14].zfill(14)\n","\n","# Apply the cleaning function to the 'CNPJ_FUNDO' column\n","df_cleaned['CNPJ_FUNDO'] = df_cleaned['CNPJ_FUNDO'].apply(clean_cnpj)\n","\n","# Initialize flag columns with False\n","flag_columns = [\n","    'ATIVO_ZERO_FLAG',\n","    'DIVERGE_LIQ_FLAG',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG',\n","    'DIVERGE_NPL_FLAG',\n","    'SEM_POSICAO_FLAG'\n","]\n","\n","for col in flag_columns:\n","    df_cleaned[col] = False\n","\n","print(\"âœ… fidc_dados_v1 (2).csv loaded, 'CNPJ_FUNDO' cleaned, and flag columns initialized!\")\n","print(f\"   Shape of df_cleaned: {df_cleaned.shape}\")\n","print(f\"   First 5 rows of df_cleaned:\\n{df_cleaned.head()}\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… fidc_dados_v1 (2).csv loaded, 'CNPJ_FUNDO' cleaned, and flag columns initialized!\n","   Shape of df_cleaned: (441, 30)\n","   First 5 rows of df_cleaned:\n","       CNPJ_FUNDO  DATA_COMPETENCIA  ATIVO_TOTAL  DISPONIBILIDADES  \\\n","0  51199121000145               0.0   3869083.39          15471.64   \n","1  47388724000118               0.0   1457082.83            258.49   \n","2  52362629000185               0.0  12975568.17            409.85   \n","3  50356663000111               0.0   2373617.44         368597.06   \n","4  41609997000169               0.0   3902880.56            416.44   \n","\n","   CARTEIRA_TOTAL  CREDITOS_ADQUIRIDOS  CRED_VENCIDOS_ADIMPLENTES  \\\n","0      3853611.75                 0.00                       0.00   \n","1      1456824.34                 0.00                 1968702.86   \n","2     12973012.42                 0.00                       0.00   \n","3      1923596.49           1923596.49                 1903382.03   \n","4      3899837.38           3728862.17                 5193721.50   \n","\n","   CRED_VENCIDOS_INADIMPLENTES  CRED_INADIMPLENCIA  DICRED_TOTAL  ...  \\\n","0                          0.0                0.00    3443588.77  ...   \n","1                          0.0           974472.84     425162.92  ...   \n","2                          0.0                0.00          0.00  ...   \n","3                          0.0            20214.46          0.00  ...   \n","4                          0.0                0.00          0.00  ...   \n","\n","   CARTEIRA_BRUTA  PROVISOES_TOTAL   STATUS  DATA_REFERENCIA_DOC  TIPO_COLETA  \\\n","0      3443588.77       -410022.98  SUCESSO              11/2025   ULTIMO_MES   \n","1       425162.92      -1031661.42  SUCESSO              11/2025   ULTIMO_MES   \n","2            0.00     -12973012.42  SUCESSO              11/2025   ULTIMO_MES   \n","3      1923596.49             0.00  SUCESSO              11/2025   ULTIMO_MES   \n","4      3728862.17       -170975.21  SUCESSO              11/2025   ULTIMO_MES   \n","\n","   ATIVO_ZERO_FLAG  DIVERGE_LIQ_FLAG  CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG  \\\n","0            False             False                              False   \n","1            False             False                              False   \n","2            False             False                              False   \n","3            False             False                              False   \n","4            False             False                              False   \n","\n","   DIVERGE_NPL_FLAG  SEM_POSICAO_FLAG  \n","0             False             False  \n","1             False             False  \n","2             False             False  \n","3             False             False  \n","4             False             False  \n","\n","[5 rows x 30 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"5c07b47a"},"source":["## Convert Numeric and Date Columns\n","\n","### Subtask:\n","Convert relevant numeric columns to float, handling PT-BR decimal parsing and '-' characters. Convert 'DATA_REFERENCIA_DOC' to a proper datetime format.\n"]},{"cell_type":"markdown","metadata":{"id":"c45d8e8a"},"source":["**Reasoning**:\n","I need to convert specified numeric columns to float and the 'DATA_REFERENCIA_DOC' column to datetime objects, handling potential errors and specific formatting as requested in the subtask.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8879c15","executionInfo":{"status":"ok","timestamp":1765818245761,"user_tz":180,"elapsed":293,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"9ad02212-d27c-434a-bd1a-334b7fe0c29e"},"source":["numeric_cols = [\n","    'DATA_COMPETENCIA',\n","    'ATIVO_TOTAL',\n","    'DISPONIBILIDADES',\n","    'CARTEIRA_TOTAL',\n","    'CREDITOS_ADQUIRIDOS',\n","    'CRED_VENCIDOS_ADIMPLENTES',\n","    'CRED_VENCIDOS_INADIMPLENTES',\n","    'CRED_INADIMPLENCIA',\n","    'DICRED_TOTAL',\n","    'DICRED_INADIMPLENCIA',\n","    'SEGMT_INDUSTRIAL',\n","    'SEGMT_AGRONEGOCIO',\n","    'SEGMT_ACAO_JUDICIAL',\n","    'SEGMT_FINANC_CORPORATIVO',\n","    'SEGMT_PRECATORIOS',\n","    'INADIMPLENCIA_TOTAL',\n","    'INDICE_NPL_DECIMAL',\n","    'TAXA_LIQUIDEZ_DECIMAL',\n","    'CONCENTRACAO_CREDITO_DECIMAL',\n","    'CARTEIRA_BRUTA',\n","    'PROVISOES_TOTAL'\n","]\n","\n","for col in numeric_cols:\n","    if col in df_cleaned.columns:\n","        # Ensure the column is treated as string before replacing, to handle mixed types safely\n","        df_cleaned[col] = df_cleaned[col].astype(str).str.replace('-', '0', regex=False)\n","        df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n","    else:\n","        print(f\"Column '{col}' not found in DataFrame. Skipping conversion.\")\n","\n","# Convert 'DATA_REFERENCIA_DOC' to datetime\n","# The format is 'MM/YYYY', so it should be '%m/%Y'\n","df_cleaned['DATA_REFERENCIA_DOC'] = pd.to_datetime(\n","    df_cleaned['DATA_REFERENCIA_DOC'],\n","    format='%m/%Y', # Expected format 'MM/YYYY'\n","    errors='coerce'\n",")\n","\n","print(\"âœ… Numeric columns converted to float and 'DATA_REFERENCIA_DOC' converted to datetime!\")\n","print(f\"   DataFrame info after conversion:\")\n","df_cleaned.info()"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Numeric columns converted to float and 'DATA_REFERENCIA_DOC' converted to datetime!\n","   DataFrame info after conversion:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 441 entries, 0 to 440\n","Data columns (total 30 columns):\n"," #   Column                             Non-Null Count  Dtype         \n","---  ------                             --------------  -----         \n"," 0   CNPJ_FUNDO                         441 non-null    object        \n"," 1   DATA_COMPETENCIA                   441 non-null    float64       \n"," 2   ATIVO_TOTAL                        441 non-null    float64       \n"," 3   DISPONIBILIDADES                   441 non-null    float64       \n"," 4   CARTEIRA_TOTAL                     441 non-null    float64       \n"," 5   CREDITOS_ADQUIRIDOS                441 non-null    float64       \n"," 6   CRED_VENCIDOS_ADIMPLENTES          441 non-null    float64       \n"," 7   CRED_VENCIDOS_INADIMPLENTES        441 non-null    float64       \n"," 8   CRED_INADIMPLENCIA                 441 non-null    float64       \n"," 9   DICRED_TOTAL                       441 non-null    float64       \n"," 10  DICRED_INADIMPLENCIA               441 non-null    float64       \n"," 11  SEGMT_INDUSTRIAL                   441 non-null    float64       \n"," 12  SEGMT_AGRONEGOCIO                  441 non-null    float64       \n"," 13  SEGMT_ACAO_JUDICIAL                441 non-null    float64       \n"," 14  SEGMT_FINANC_CORPORATIVO           441 non-null    float64       \n"," 15  SEGMT_PRECATORIOS                  441 non-null    float64       \n"," 16  INADIMPLENCIA_TOTAL                441 non-null    float64       \n"," 17  INDICE_NPL_DECIMAL                 441 non-null    float64       \n"," 18  TAXA_LIQUIDEZ_DECIMAL              441 non-null    float64       \n"," 19  CONCENTRACAO_CREDITO_DECIMAL       441 non-null    float64       \n"," 20  CARTEIRA_BRUTA                     441 non-null    float64       \n"," 21  PROVISOES_TOTAL                    441 non-null    float64       \n"," 22  STATUS                             441 non-null    object        \n"," 23  DATA_REFERENCIA_DOC                441 non-null    datetime64[ns]\n"," 24  TIPO_COLETA                        441 non-null    object        \n"," 25  ATIVO_ZERO_FLAG                    441 non-null    bool          \n"," 26  DIVERGE_LIQ_FLAG                   441 non-null    bool          \n"," 27  CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG  441 non-null    bool          \n"," 28  DIVERGE_NPL_FLAG                   441 non-null    bool          \n"," 29  SEM_POSICAO_FLAG                   441 non-null    bool          \n","dtypes: bool(5), datetime64[ns](1), float64(21), object(3)\n","memory usage: 88.4+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"bf208be2"},"source":["## Calculate and Validate Liquidez\n","\n","### Subtask:\n","Calculate `liquidez_calc` as `DISPONIBILIDADES / ATIVO_TOTAL`. Apply the `ATIVO_ZERO_FLAG` where `ATIVO_TOTAL` is zero. For `ATIVO_TOTAL > 0`, validate if `round(liquidez_calc, 2)` matches `TAXA_LIQUIDEZ_DECIMAL` (after converting both to 2 decimal places), setting `DIVERGE_LIQ_FLAG` if they differ.\n"]},{"cell_type":"markdown","metadata":{"id":"0edbb3a3"},"source":["**Reasoning**:\n","I will calculate `liquidez_calc` while handling division by zero, set the `ATIVO_ZERO_FLAG` for records where `ATIVO_TOTAL` is zero, and then validate `liquidez_calc` against `TAXA_LIQUIDEZ_DECIMAL` by setting `DIVERGE_LIQ_FLAG` based on rounded values.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"c112a19a","executionInfo":{"status":"error","timestamp":1765818272106,"user_tz":180,"elapsed":25,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"715ceb80-1428-4cd0-ac49-4311a58e7375"},"source":["import numpy as np\n","\n","# 1. Calculate liquidez_calc, handling division by zero\n","df_cleaned['liquidez_calc'] = np.where(\n","    df_cleaned['ATIVO_TOTAL'] == 0,\n","    0, # Set to 0 if ATIVO_TOTAL is 0, or could be np.nan\n","    df_cleaned['DISPONIBILIDADES'] / df_cleaned['ATIVO_TOTAL']\n",")\n","\n","# 2. Set ATIVO_ZERO_FLAG where ATIVO_TOTAL is zero\n","df_cleaned.loc[df_cleaned['ATIVO_TOTAL'] == 0, 'ATIVO_ZERO_FLAG'] = True\n","\n","# 3. Validate liquidez_calc against TAXA_LIQUIDEZ_DECIMAL\n","# Only for rows where ATIVO_TOTAL > 0\n","condition_for_validation = df_cleaned['ATIVO_TOTAL'] > 0\n","\n","df_cleaned.loc[condition_for_validation, 'DIVERGE_LIQ_FLAG'] = (\n","    round(df_cleaned['liquidez_calc'], 2) !=\n","    round(df_cleaned['TAXA_LIQUIDEZ_DECIMAL'], 2)\n",").astype(bool)\n","\n","print(\"âœ… 'liquidez_calc' calculated, 'ATIVO_ZERO_FLAG' and 'DIVERGE_LIQ_FLAG' updated!\")\n","print(f\"   Number of records with ATIVO_ZERO_FLAG: {df_cleaned['ATIVO_ZERO_FLAG'].sum()}\")\n","print(f\"   Number of records with DIVERGE_LIQ_FLAG: {df_cleaned['DIVERGE_LIQ_FLAG'].sum()}\")\n","print(f\"   First 5 rows with new columns:\n","{df_cleaned[['ATIVO_TOTAL', 'DISPONIBILIDADES', 'TAXA_LIQUIDEZ_DECIMAL', 'liquidez_calc', 'ATIVO_ZERO_FLAG', 'DIVERGE_LIQ_FLAG']].head()}\")"],"execution_count":21,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unterminated f-string literal (detected at line 25) (ipython-input-63303973.py, line 25)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-63303973.py\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    print(f\"   First 5 rows with new columns:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated f-string literal (detected at line 25)\n"]}]},{"cell_type":"markdown","metadata":{"id":"625c22da"},"source":["**Reasoning**:\n","The previous code failed due to an `unterminated f-string literal` in the print statement. I need to correct the f-string to ensure it is properly terminated by placing the entire output on a single line.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fafca29","executionInfo":{"status":"ok","timestamp":1765818299516,"user_tz":180,"elapsed":43,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"5b70c0f7-facf-42a0-f3fa-8c0252e05dfa"},"source":["import numpy as np\n","\n","# 1. Calculate liquidez_calc, handling division by zero\n","df_cleaned['liquidez_calc'] = np.where(\n","    df_cleaned['ATIVO_TOTAL'] == 0,\n","    0, # Set to 0 if ATIVO_TOTAL is 0, or could be np.nan\n","    df_cleaned['DISPONIBILIDADES'] / df_cleaned['ATIVO_TOTAL']\n",")\n","\n","# 2. Set ATIVO_ZERO_FLAG where ATIVO_TOTAL is zero\n","df_cleaned.loc[df_cleaned['ATIVO_TOTAL'] == 0, 'ATIVO_ZERO_FLAG'] = True\n","\n","# 3. Validate liquidez_calc against TAXA_LIQUIDEZ_DECIMAL\n","# Only for rows where ATIVO_TOTAL > 0\n","condition_for_validation = df_cleaned['ATIVO_TOTAL'] > 0\n","\n","df_cleaned.loc[condition_for_validation, 'DIVERGE_LIQ_FLAG'] = (\n","    round(df_cleaned['liquidez_calc'], 2) !=\n","    round(df_cleaned['TAXA_LIQUIDEZ_DECIMAL'], 2)\n",").astype(bool)\n","\n","print(\"âœ… 'liquidez_calc' calculated, 'ATIVO_ZERO_FLAG' and 'DIVERGE_LIQ_FLAG' updated!\")\n","print(f\"   Number of records with ATIVO_ZERO_FLAG: {df_cleaned['ATIVO_ZERO_FLAG'].sum()}\")\n","print(f\"   Number of records with DIVERGE_LIQ_FLAG: {df_cleaned['DIVERGE_LIQ_FLAG'].sum()}\")\n","print(f\"   First 5 rows with new columns:\\n{df_cleaned[['ATIVO_TOTAL', 'DISPONIBILIDADES', 'TAXA_LIQUIDEZ_DECIMAL', 'liquidez_calc', 'ATIVO_ZERO_FLAG', 'DIVERGE_LIQ_FLAG']].head()}\")"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 'liquidez_calc' calculated, 'ATIVO_ZERO_FLAG' and 'DIVERGE_LIQ_FLAG' updated!\n","   Number of records with ATIVO_ZERO_FLAG: 3\n","   Number of records with DIVERGE_LIQ_FLAG: 0\n","   First 5 rows with new columns:\n","   ATIVO_TOTAL  DISPONIBILIDADES  TAXA_LIQUIDEZ_DECIMAL  liquidez_calc  \\\n","0   3869083.39          15471.64                   0.00       0.003999   \n","1   1457082.83            258.49                   0.00       0.000177   \n","2  12975568.17            409.85                   0.00       0.000032   \n","3   2373617.44         368597.06                   0.16       0.155289   \n","4   3902880.56            416.44                   0.00       0.000107   \n","\n","   ATIVO_ZERO_FLAG  DIVERGE_LIQ_FLAG  \n","0            False             False  \n","1            False             False  \n","2            False             False  \n","3            False             False  \n","4            False             False  \n"]}]},{"cell_type":"markdown","metadata":{"id":"1ad4488c"},"source":["## Calculate and Validate NPL\n","\n","### Subtask:\n","Calculate `npl_calc` as `INADIMPLENCIA_TOTAL / CARTEIRA_BRUTA`. Set `CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG` where `CARTEIRA_BRUTA` is zero and `INADIMPLENCIA_TOTAL` is greater than zero, ensuring `INDICE_NPL_DECIMAL` becomes NaN in these cases. For `CARTEIRA_BRUTA > 0`, validate if the absolute difference between `npl_calc` and `INDICE_NPL_DECIMAL` is greater than `0.01`, setting `DIVERGE_NPL_FLAG` if it is.\n"]},{"cell_type":"markdown","metadata":{"id":"6b355d60"},"source":["**Reasoning**:\n","I need to calculate 'npl_calc', identify and flag cases where 'CARTEIRA_BRUTA' is zero but 'INADIMPLENCIA_TOTAL' is not, set 'INDICE_NPL_DECIMAL' to NaN for these cases, and then validate 'npl_calc' against 'INDICE_NPL_DECIMAL' based on an absolute difference threshold as per the subtask instructions. Finally, I'll print summaries and display relevant columns.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e10e9c3d","executionInfo":{"status":"ok","timestamp":1765818324830,"user_tz":180,"elapsed":46,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"5cb5a4f2-9e23-4e31-a8e1-3458a80f24c0"},"source":["import numpy as np\n","\n","# 1. Calculate npl_calc, handling division by zero\n","df_cleaned['npl_calc'] = np.where(\n","    df_cleaned['CARTEIRA_BRUTA'] == 0,\n","    0, # Default to 0 if CARTEIRA_BRUTA is 0\n","    df_cleaned['INADIMPLENCIA_TOTAL'] / df_cleaned['CARTEIRA_BRUTA']\n",")\n","\n","# 2. Identify rows where CARTEIRA_BRUTA is 0 AND INADIMPLENCIA_TOTAL > 0\n","condition_carteira_bruta_zero_com_inad = (\n","    (df_cleaned['CARTEIRA_BRUTA'] == 0) &\n","    (df_cleaned['INADIMPLENCIA_TOTAL'] > 0)\n",")\n","df_cleaned.loc[condition_carteira_bruta_zero_com_inad, 'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG'] = True\n","\n","# 3. For these flagged rows, set npl_calc and INDICE_NPL_DECIMAL to NaN\n","df_cleaned.loc[condition_carteira_bruta_zero_com_inad, ['npl_calc', 'INDICE_NPL_DECIMAL']] = np.nan\n","\n","# 4. Validate npl_calc against INDICE_NPL_DECIMAL for CARTEIRA_BRUTA > 0\n","# Exclude cases where CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG is True because NPL is NaN there\n","condition_for_npl_validation = df_cleaned['CARTEIRA_BRUTA'] > 0\n","\n","df_cleaned.loc[condition_for_npl_validation, 'DIVERGE_NPL_FLAG'] = (\n","    np.abs(df_cleaned['npl_calc'] - df_cleaned['INDICE_NPL_DECIMAL']) > 0.01\n",")\n","\n","# 5. Print counts\n","print(\"âœ… 'npl_calc' calculated, flags updated!\")\n","print(f\"   Number of records with CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG: {df_cleaned['CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG'].sum()}\")\n","print(f\"   Number of records with DIVERGE_NPL_FLAG: {df_cleaned['DIVERGE_NPL_FLAG'].sum()}\")\n","\n","# 6. Display first 5 rows with relevant columns\n","print(\"\\n   First 5 rows with new columns:\")\n","print(df_cleaned[[\n","    'INADIMPLENCIA_TOTAL', 'CARTEIRA_BRUTA', 'INDICE_NPL_DECIMAL', 'npl_calc',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG', 'DIVERGE_NPL_FLAG'\n","]].head())\n"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 'npl_calc' calculated, flags updated!\n","   Number of records with CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG: 5\n","   Number of records with DIVERGE_NPL_FLAG: 0\n","\n","   First 5 rows with new columns:\n","   INADIMPLENCIA_TOTAL  CARTEIRA_BRUTA  INDICE_NPL_DECIMAL   npl_calc  \\\n","0             95676.06      3443588.77                0.03   0.027784   \n","1           4648786.29       425162.92               10.93  10.934129   \n","2                 0.00            0.00                0.00   0.000000   \n","3             20214.46      1923596.49                0.01   0.010509   \n","4                 0.00      3728862.17                0.00   0.000000   \n","\n","   CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG  DIVERGE_NPL_FLAG  \n","0                              False             False  \n","1                              False             False  \n","2                              False             False  \n","3                              False             False  \n","4                              False             False  \n"]}]},{"cell_type":"markdown","metadata":{"id":"056dabb9"},"source":["**Reasoning**:\n","I need to calculate 'CARTEIRA_LIQUIDA_CALC' and then set the 'SEM_POSICAO_FLAG' based on the conditions specified in the subtask.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"646711fe","executionInfo":{"status":"ok","timestamp":1765818332688,"user_tz":180,"elapsed":48,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"70db91d3-504e-4b92-bfab-22024eaac848"},"source":["import numpy as np\n","\n","# 1. Calculate CARTEIRA_LIQUIDA_CALC\n","df_cleaned['CARTEIRA_LIQUIDA_CALC'] = df_cleaned['CREDITOS_ADQUIRIDOS'] + df_cleaned['DICRED_TOTAL']\n","\n","# 2. Set SEM_POSICAO_FLAG where CARTEIRA_LIQUIDA_CALC is 0 AND ATIVO_TOTAL > 0\n","condition_sem_posicao = (\n","    (df_cleaned['CARTEIRA_LIQUIDA_CALC'] == 0) &\n","    (df_cleaned['ATIVO_TOTAL'] > 0)\n",")\n","df_cleaned.loc[condition_sem_posicao, 'SEM_POSICAO_FLAG'] = True\n","\n","print(\"âœ… 'CARTEIRA_LIQUIDA_CALC' calculated and 'SEM_POSICAO_FLAG' updated!\")\n","print(f\"   Number of records with SEM_POSICAO_FLAG: {df_cleaned['SEM_POSICAO_FLAG'].sum()}\")\n","print(\"   First 5 rows with new columns:\")\n","print(df_cleaned[[\n","    'CREDITOS_ADQUIRIDOS', 'DICRED_TOTAL', 'CARTEIRA_LIQUIDA_CALC',\n","    'ATIVO_TOTAL', 'SEM_POSICAO_FLAG'\n","]].head())\n"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 'CARTEIRA_LIQUIDA_CALC' calculated and 'SEM_POSICAO_FLAG' updated!\n","   Number of records with SEM_POSICAO_FLAG: 46\n","   First 5 rows with new columns:\n","   CREDITOS_ADQUIRIDOS  DICRED_TOTAL  CARTEIRA_LIQUIDA_CALC  ATIVO_TOTAL  \\\n","0                 0.00    3443588.77             3443588.77   3869083.39   \n","1                 0.00     425162.92              425162.92   1457082.83   \n","2                 0.00          0.00                   0.00  12975568.17   \n","3           1923596.49          0.00             1923596.49   2373617.44   \n","4           3728862.17          0.00             3728862.17   3902880.56   \n","\n","   SEM_POSICAO_FLAG  \n","0             False  \n","1             False  \n","2              True  \n","3             False  \n","4             False  \n"]}]},{"cell_type":"markdown","metadata":{"id":"0a8c7637"},"source":["## Generate QA Issues Report\n","\n","### Subtask:\n","Filter the main DataFrame to include only rows where any of the validation flags (`ATIVO_ZERO_FLAG`, `DIVERGE_LIQ_FLAG`, `CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG`, `DIVERGE_NPL_FLAG`, `SEM_POSICAO_FLAG`) are True. Export this filtered DataFrame to `qa_issues.csv`.\n"]},{"cell_type":"markdown","metadata":{"id":"a7bbeb05"},"source":["**Reasoning**:\n","I need to filter the `df_cleaned` DataFrame based on the validation flags, create a `df_qa_issues` DataFrame, and then export it to a CSV file with the specified formatting.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e102fb20","executionInfo":{"status":"ok","timestamp":1765818354352,"user_tz":180,"elapsed":40,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"3205802c-c0cf-4e72-f496-c626bcf6ed8b"},"source":["import pandas as pd\n","\n","# 1. Define a list of validation flag columns\n","validation_flags = [\n","    'ATIVO_ZERO_FLAG',\n","    'DIVERGE_LIQ_FLAG',\n","    'CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG',\n","    'DIVERGE_NPL_FLAG',\n","    'SEM_POSICAO_FLAG'\n","]\n","\n","# 2. Create a boolean Series for rows with any True flag\n","has_issues = df_cleaned[validation_flags].any(axis=1)\n","\n","# 3. Filter df_cleaned to create df_qa_issues\n","df_qa_issues = df_cleaned[has_issues].copy()\n","\n","# 4. Export df_qa_issues to 'qa_issues.csv'\n","qa_issues_filename = 'qa_issues.csv'\n","df_qa_issues.to_csv(\n","    qa_issues_filename,\n","    sep=';',\n","    decimal=',',\n","    index=False,\n","    encoding='utf-8-sig',\n","    float_format='%.2f'\n",")\n","\n","print(f\"âœ… QA issues report generated: {qa_issues_filename}\")\n","print(f\"   Number of rows with issues: {len(df_qa_issues)}\")\n","print(f\"   First 5 rows of QA issues:\\n{df_qa_issues.head()}\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… QA issues report generated: qa_issues.csv\n","   Number of rows with issues: 49\n","   First 5 rows of QA issues:\n","        CNPJ_FUNDO  DATA_COMPETENCIA   ATIVO_TOTAL  DISPONIBILIDADES  \\\n","2   52362629000185               0.0  1.297557e+07            409.85   \n","36  35689357000103               0.0  1.422617e+08              0.00   \n","46  48960844000100               0.0  1.415174e+05            100.00   \n","54  18048642000126               0.0  2.428103e+09              0.00   \n","57  34218194000118               0.0  3.630317e+07            100.00   \n","\n","    CARTEIRA_TOTAL  CREDITOS_ADQUIRIDOS  CRED_VENCIDOS_ADIMPLENTES  \\\n","2     1.297301e+07                  0.0                        0.0   \n","36    1.422600e+08                  0.0                        0.0   \n","46    1.414124e+05                  0.0                        0.0   \n","54    2.428103e+09                  0.0                        0.0   \n","57    7.415000e+01                  0.0                        0.0   \n","\n","    CRED_VENCIDOS_INADIMPLENTES  CRED_INADIMPLENCIA  DICRED_TOTAL  ...  \\\n","2                           0.0                 0.0           0.0  ...   \n","36                          0.0                 0.0           0.0  ...   \n","46                          0.0                 0.0           0.0  ...   \n","54                          0.0                 0.0           0.0  ...   \n","57                          0.0                 0.0           0.0  ...   \n","\n","    DATA_REFERENCIA_DOC  TIPO_COLETA  ATIVO_ZERO_FLAG  DIVERGE_LIQ_FLAG  \\\n","2            2025-11-01   ULTIMO_MES            False             False   \n","36           2025-10-01   ULTIMO_MES            False             False   \n","46           2025-11-01   ULTIMO_MES            False             False   \n","54           2025-10-01   ULTIMO_MES            False             False   \n","57           2025-11-01   ULTIMO_MES            False             False   \n","\n","    CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG  DIVERGE_NPL_FLAG  SEM_POSICAO_FLAG  \\\n","2                               False             False              True   \n","36                              False             False              True   \n","46                              False             False              True   \n","54                              False             False              True   \n","57                              False             False              True   \n","\n","    liquidez_calc  npl_calc  CARTEIRA_LIQUIDA_CALC  \n","2        0.000032       0.0                    0.0  \n","36       0.000000       0.0                    0.0  \n","46       0.000707       0.0                    0.0  \n","54       0.000000       0.0                    0.0  \n","57       0.000003       0.0                    0.0  \n","\n","[5 rows x 33 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"5f5a3a55"},"source":["## Export Cleaned Snapshot\n","\n","### Subtask:\n","Export the fully cleaned and validated DataFrame (including all calculated columns and flags) to `cleaned_snapshot.csv`, ensuring numeric columns are properly formatted without scientific notation and using `;` as separator and `,` as decimal point.\n"]},{"cell_type":"markdown","metadata":{"id":"a363ec10"},"source":["**Reasoning**:\n","I need to export the `df_cleaned` DataFrame to a CSV file named `cleaned_snapshot.csv` with the specified formatting (no index, semicolon separator, comma decimal point, two decimal places for floats, and 'utf-8-sig' encoding).\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2a506e9","executionInfo":{"status":"ok","timestamp":1765818371482,"user_tz":180,"elapsed":65,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"f066bba3-6aec-42b4-92ff-24a2a716237d"},"source":["cleaned_snapshot_filename = 'cleaned_snapshot.csv'\n","df_cleaned.to_csv(\n","    cleaned_snapshot_filename,\n","    index=False,\n","    sep=';',\n","    decimal=',',\n","    float_format='%.2f',\n","    encoding='utf-8-sig'\n",")\n","\n","print(f\"âœ… Cleaned snapshot exported to '{cleaned_snapshot_filename}'\")\n","print(f\"   Shape of exported data: {df_cleaned.shape}\")"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Cleaned snapshot exported to 'cleaned_snapshot.csv'\n","   Shape of exported data: (441, 33)\n"]}]},{"cell_type":"markdown","metadata":{"id":"96565592"},"source":["## Define Optional Diff Function\n","\n","### Subtask:\n","Create a Python function that accepts two DataFrames (representing two versions of the data). This function will compare numerical columns between the two DataFrames for common CNPJs and identify differences, outputting a `diff_v1_v2.csv` file with the CNPJ, column name, and numeric difference. This function will only be executed if the user explicitly requests it.\n"]},{"cell_type":"markdown","metadata":{"id":"b7eff1d0"},"source":["## Define Optional Diff Function\n","\n","### Subtask:\n","Create a Python function that accepts two DataFrames (representing two versions of the data). This function will compare numerical columns between the two DataFrames for common CNPJs and identify differences, outputting a `diff_v1_v2.csv` file with the CNPJ, column name, and numeric difference. This function will only be executed if the user explicitly requests it.\n","\n","#### Instructions\n","1. Define a function, for example, `generate_diff_report`, that takes two pandas DataFrames as input, representing the original and new versions of the data.\n","2. Inside the function, identify common CNPJs between the two DataFrames.\n","3. Iterate through the numerical columns present in both DataFrames.\n","4. For each common CNPJ and numerical column, calculate the difference between the values in the two DataFrames.\n","5. Store these differences along with the CNPJ and column name in a new DataFrame.\n","6. Export this new DataFrame to a CSV file named `diff_v1_v2.csv`, using `;` as a separator, `,` as a decimal point, and formatting float values to two decimal places.\n","\n","**Reasoning**:\n","This function, `generate_diff_report`, will be crucial for performing quality assurance by comparing the newly processed `cleaned_snapshot.csv` with a previous version (`fidc_dados_v1.csv` is used here as a placeholder for a 'previous version' for demonstration, although in a real scenario it would be a distinct 'v1' to compare with a 'v2'). This allows us to quickly identify any discrepancies in numerical values, which is essential for maintaining data integrity and ensuring that transformations are applied correctly. The report will highlight differences by CNPJ and column, facilitating targeted investigation of data changes."]},{"cell_type":"markdown","metadata":{"id":"eeffb2cf"},"source":["## Define Optional Diff Function\n","\n","### Subtask:\n","Create a Python function that accepts two DataFrames (representing two versions of the data). This function will compare numerical columns between the two DataFrames for common CNPJs and identify differences, outputting a `diff_v1_v2.csv` file with the CNPJ, column name, and numeric difference. This function will only be executed if the user explicitly requests it.\n","\n","#### Instructions\n","1. Define a function, for example, `generate_diff_report`, that takes two pandas DataFrames as input, representing the original and new versions of the data.\n","2. Inside the function, identify common CNPJs between the two DataFrames.\n","3. Iterate through the numerical columns present in both DataFrames.\n","4. For each common CNPJ and numerical column, calculate the difference between the values in the two DataFrames.\n","5. Store these differences along with the CNPJ and column name in a new DataFrame.\n","6. Export this new DataFrame to a CSV file named `diff_v1_v2.csv`, using `;` as a separator, `,` as a decimal point, and formatting float values to two decimal places.\n","\n","**Reasoning**:\n","This function, `generate_diff_report`, will be crucial for performing quality assurance by comparing the newly processed `cleaned_snapshot.csv` with a previous version (`fidc_dados_v1.csv` is used here as a placeholder for a 'previous version' for demonstration, although in a real scenario it would be a distinct 'v1' to compare with a 'v2'). This allows us to quickly identify any discrepancies in numerical values, which is essential for maintaining data integrity and ensuring that transformations are applied correctly. The report will highlight differences by CNPJ and column, facilitating targeted investigation of data changes."]},{"cell_type":"markdown","metadata":{"id":"d8120a84"},"source":["## Define Optional Diff Function\n","\n","### Subtask:\n","Create a Python function that accepts two DataFrames (representing two versions of the data). This function will compare numerical columns between the two DataFrames for common CNPJs and identify differences, outputting a `diff_v1_v2.csv` file with the CNPJ, column name, and numeric difference. This function will only be executed if the user explicitly requests it.\n","\n","#### Instructions\n","1. Define a function, for example, `generate_diff_report`, that takes two pandas DataFrames as input, representing the original and new versions of the data.\n","2. Inside the function, identify common CNPJs between the two DataFrames.\n","3. Iterate through the numerical columns present in both DataFrames.\n","4. For each common CNPJ and numerical column, calculate the difference between the values in the two DataFrames.\n","5. Store these differences along with the CNPJ and column name in a new DataFrame.\n","6. Export this new DataFrame to a CSV file named `diff_v1_v2.csv`, using `;` as a separator, `,` as a decimal point, and formatting float values to two decimal places.\n","\n","**Reasoning**:\n","This function, `generate_diff_report`, will be crucial for performing quality assurance by comparing the newly processed `cleaned_snapshot.csv` with a previous version (`fidc_dados_v1.csv` is used here as a placeholder for a 'previous version' for demonstration, although in a real scenario it would be a distinct 'v1' to compare with a 'v2'). This allows us to quickly identify any discrepancies in numerical values, which is essential for maintaining data integrity and ensuring that transformations are applied correctly. The report will highlight differences by CNPJ and column, facilitating targeted investigation of data changes."]},{"cell_type":"markdown","metadata":{"id":"d6a3aa39"},"source":["**Reasoning**:\n","I need to provide the actual Python code for the `generate_diff_report` function as described in the previous markdown block. This function will take two DataFrames, find common CNPJs, compare numerical columns, and export the differences to a CSV file.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d19988a","executionInfo":{"status":"ok","timestamp":1765818425829,"user_tz":180,"elapsed":45,"user":{"displayName":"Rafael Augusto","userId":"13656587941602724451"}},"outputId":"ad830f3c-52fc-439b-8a01-556520ab3d6a"},"source":["def generate_diff_report(df_v1: pd.DataFrame, df_v2: pd.DataFrame) -> None:\n","    \"\"\"\n","    Compares numerical columns between two DataFrames (df_v1 and df_v2) for common CNPJs,\n","    identifies differences, and outputs a CSV file with the CNPJ, column name, and numeric difference.\n","\n","    Args:\n","        df_v1: The first DataFrame (e.g., original data).\n","        df_v2: The second DataFrame (e.g., new/cleaned data).\n","    \"\"\"\n","\n","    # Ensure CNPJ_FUNDO is the index for easier alignment\n","    df_v1_indexed = df_v1.set_index('CNPJ_FUNDO')\n","    df_v2_indexed = df_v2.set_index('CNPJ_FUNDO')\n","\n","    # Identify common CNPJs\n","    common_cnpjs = list(set(df_v1_indexed.index) & set(df_v2_indexed.index))\n","\n","    if not common_cnpjs:\n","        print(\"No common CNPJs found between the two DataFrames. No diff report generated.\")\n","        return\n","\n","    # Filter to common CNPJs\n","    df_v1_common = df_v1_indexed.loc[common_cnpjs]\n","    df_v2_common = df_v2_indexed.loc[common_cnpjs]\n","\n","    # Identify common numerical columns\n","    numeric_cols_v1 = df_v1_common.select_dtypes(include=np.number).columns\n","    numeric_cols_v2 = df_v2_common.select_dtypes(include=np.number).columns\n","    common_numeric_cols = list(set(numeric_cols_v1) & set(numeric_cols_v2))\n","\n","    if not common_numeric_cols:\n","        print(\"No common numerical columns found between the two DataFrames. No diff report generated.\")\n","        return\n","\n","    diff_records = []\n","\n","    for cnpj in common_cnpjs:\n","        for col in common_numeric_cols:\n","            val_v1 = df_v1_common.loc[cnpj, col]\n","            val_v2 = df_v2_common.loc[cnpj, col]\n","\n","            # Handle potential NaN values, consider them as 0 for difference calculation if applicable\n","            # Or exclude if any is NaN\n","            if pd.isna(val_v1) or pd.isna(val_v2):\n","                if not pd.isna(val_v1) or not pd.isna(val_v2): # If only one is NaN, it's a difference\n","                    diff = val_v2 - val_v1 if not pd.isna(val_v1) else val_v2 # If val_v1 is NaN, difference is val_v2\n","                    if not pd.isna(val_v2): # If val_v2 is NaN, difference is -val_v1\n","                        diff = val_v2 if pd.isna(val_v1) else -val_v1\n","                    else:\n","                        diff = np.nan\n","                else:\n","                    diff = 0.0 # Both NaN, no difference\n","            else:\n","                diff = val_v2 - val_v1\n","\n","            if diff != 0:\n","                diff_records.append({\n","                    'CNPJ_FUNDO': cnpj,\n","                    'COLUNA': col,\n","                    'DIFERENCA': diff\n","                })\n","\n","    if diff_records:\n","        df_diff = pd.DataFrame(diff_records)\n","        diff_filename = 'diff_v1_v2.csv'\n","        df_diff.to_csv(\n","            diff_filename,\n","            index=False,\n","            sep=';',\n","            decimal=',',\n","            float_format='%.2f',\n","            encoding='utf-8-sig'\n","        )\n","        print(f\"âœ… Difference report generated: {diff_filename} ({len(df_diff)} differences found).\")\n","    else:\n","        print(\"No numerical differences found between the two DataFrames.\")\n","\n","print(\"âœ… `generate_diff_report` function defined!\")"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… `generate_diff_report` function defined!\n"]}]},{"cell_type":"markdown","metadata":{"id":"bd09f344"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the performed actions, the generated output files (`qa_issues.csv` and `cleaned_snapshot.csv`), and explain how to use the optional diff function. Confirm that the script now adheres to all specified audit observations.\n"]},{"cell_type":"markdown","metadata":{"id":"5460b739"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   **Initial Data Preparation**: The `fidc_dados_v1 (2).csv` file was successfully loaded. The 'CNPJ\\_FUNDO' column was cleaned to a standardized 14-digit string format.\n","*   **Data Type Conversion**: 21 numeric columns were successfully converted to `float64`, handling specific parsing rules (e.g., replacing '-' with '0'). The 'DATA\\_REFERENCIA\\_DOC' column was converted to a `datetime64[ns]` format.\n","*   **Liquidez Calculation and Validation**:\n","    *   `liquidez_calc` was computed based on 'DISPONIBILIDADES' and 'ATIVO\\_TOTAL'.\n","    *   The `ATIVO_ZERO_FLAG` was set to `True` for 3 records where 'ATIVO\\_TOTAL' was zero.\n","    *   No discrepancies were found (0 records flagged) for `DIVERGE_LIQ_FLAG` when comparing `liquidez_calc` with `TAXA_LIQUIDEZ_DECIMAL` after rounding to two decimal places.\n","*   **NPL Calculation and Validation**:\n","    *   `npl_calc` was computed from 'INADIMPLENCIA\\_TOTAL' and 'CARTEIRA\\_BRUTA'.\n","    *   The `CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG` was set to `True` for 5 records where 'CARTEIRA\\_BRUTA' was zero but 'INADIMPLENCIA\\_TOTAL' was greater than zero; for these, `npl_calc` and `INDICE_NPL_DECIMAL` were set to `NaN`.\n","    *   No discrepancies were found (0 records flagged) for `DIVERGE_NPL_FLAG` based on an absolute difference threshold of 0.01 between `npl_calc` and `INDICE_NPL_DECIMAL`.\n","*   **Portfolio Liquidity Flag**:\n","    *   `CARTEIRA_LIQUIDA_CALC` was calculated as the sum of 'CREDITOS\\_ADQUIRIDOS' and 'DICRED\\_TOTAL'.\n","    *   The `SEM_POSICAO_FLAG` was set to `True` for 46 records where `CARTEIRA_LIQUIDA_CALC` was zero and 'ATIVO\\_TOTAL' was greater than zero.\n","*   **QA Issues Report**: A `qa_issues.csv` file was generated, containing 49 rows that exhibited at least one of the defined data quality issues (i.e., any validation flag was `True`).\n","*   **Cleaned Data Export**: A `cleaned_snapshot.csv` file was successfully exported, containing the fully processed data (441 rows and 33 columns) with all new calculations and flags, using specified formatting for separators and decimal points.\n","*   **Optional Diff Function**: A Python function, `generate_diff_report`, was defined to compare numerical columns between two DataFrames based on common 'CNPJ\\_FUNDO' values and report any differences to `diff_v1_v2.csv`.\n","\n","### Insights or Next Steps\n","\n","*   The `qa_issues.csv` report (containing 49 records) should be thoroughly reviewed to understand the nature and impact of the identified data anomalies. Prioritize investigation into the 46 records flagged with `SEM_POSICAO_FLAG` and the 5 records with `CARTEIRA_BRUTA_ZERO_COM_INAD_FLAG` as these represent potential data inconsistencies or unusual financial positions.\n","*   Utilize the `generate_diff_report` function in future data processing cycles to quickly identify changes in numerical values between different versions of the cleaned data, which will be crucial for auditability and quality control, especially for changes beyond the initially defined flags.\n"]}]}
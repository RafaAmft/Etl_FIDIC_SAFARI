# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ETL FIDC V5 - VERS√ÉO OTIMIZADA PARA GOOGLE COLAB
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Melhorias implementadas:
# 1. ‚úÖ Bug fix - Convers√£o de tipos
# 2. ‚úÖ Logging profissional
# 3. ‚úÖ Retry logic com backoff exponencial
# 4. ‚úÖ Paraleliza√ß√£o (ThreadPoolExecutor)
# 5. ‚úÖ Cache de resultados
# 6. ‚úÖ Progress bar (tqdm)
# 7. ‚úÖ Valida√ß√£o robusta de dados
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 1: INSTALA√á√ÉO DE DEPEND√äNCIAS (APENAS SE NECESS√ÅRIO)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Instalar apenas se n√£o tiver (Colab j√° tem a maioria)
!pip install tenacity -q

print("‚úÖ Depend√™ncias verificadas!")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 2: IMPORTS E CONFIGURA√á√ïES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

import pandas as pd
import requests
import base64
import xml.etree.ElementTree as ET
import time
import os
import logging
import hashlib
import pickle
from typing import Dict, List, Optional, Union
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURA√á√ïES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# API B3
URL_API_BUSCA = "https://fnet.bmfbovespa.com.br/fnet/publico/pesquisarGerenciadorDocumentosDados"
URL_API_DOWNLOAD = "https://fnet.bmfbovespa.com.br/fnet/publico/downloadDocumento"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# Controle de requisi√ß√µes
DELAY_ENTRE_REQUISICOES = 2
TIMEOUT_BUSCA = 10
TIMEOUT_DOWNLOAD = 20
MAX_RETRIES = 3

# Coleta hist√≥rica
ANOS_DEZEMBRO = [2020, 2021, 2022, 2023, 2024]
COLETAR_ULTIMO_MES = True

# Performance
MAX_WORKERS = 5  # Threads paralelas
CACHE_ENABLED = True

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURA√á√ÉO DE LOGGING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Criar diret√≥rio de logs
LOG_DIR = Path('logs')
LOG_DIR.mkdir(exist_ok=True)

# Configurar logger
log_filename = LOG_DIR / f'etl_fidc_{datetime.now():%Y%m%d_%H%M%S}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Configurar cache
CACHE_DIR = Path('.cache_fidc')
if CACHE_ENABLED:
    CACHE_DIR.mkdir(exist_ok=True)
    logger.info(f"Cache habilitado em: {CACHE_DIR.absolute()}")

logger.info("="*80)
logger.info("ETL FIDC V5 - VERS√ÉO OTIMIZADA")
logger.info("="*80)
logger.info(f"Configura√ß√µes carregadas:")
logger.info(f"  ‚Ä¢ Delay entre requisi√ß√µes: {DELAY_ENTRE_REQUISICOES}s")
logger.info(f"  ‚Ä¢ Max workers (threads): {MAX_WORKERS}")
logger.info(f"  ‚Ä¢ Max retries: {MAX_RETRIES}")
logger.info(f"  ‚Ä¢ Anos dezembro: {ANOS_DEZEMBRO}")
logger.info(f"  ‚Ä¢ Cache: {'Habilitado' if CACHE_ENABLED else 'Desabilitado'}")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 3: FUN√á√ïES AUXILIARES MELHORADAS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def limpar_tag(tag: str) -> str:
    """Remove namespace XML das tags."""
    return tag.split('}')[-1] if '}' in tag else tag


def converter_valor(texto: Union[str, float, int, None]) -> float:
    """
    Converte string no formato brasileiro para float com valida√ß√£o robusta.
    
    Args:
        texto: Valor a ser convertido
        
    Returns:
        float: Valor convertido ou 0.0 se inv√°lido
    """
    if texto is None or texto == '':
        return 0.0
    
    # Se j√° √© n√∫mero, retorna
    if isinstance(texto, (int, float)):
        return float(texto)
    
    try:
        texto_str = str(texto).strip()
        if not texto_str:
            return 0.0
        
        # Remove pontos (separador de milhar) e troca v√≠rgula por ponto
        texto_limpo = texto_str.replace('.', '').replace(',', '.')
        return float(texto_limpo)
    except (ValueError, AttributeError) as e:
        logger.debug(f"Erro ao converter '{texto}': {e}")
        return 0.0


def buscar_valor_xml(root: ET.Element, caminho: str) -> Union[float, str]:
    """
    Busca um valor no XML com tratamento robusto de namespaces.
    
    Args:
        root: Elemento raiz do XML
        caminho: Caminho XPath simplificado
        
    Returns:
        float ou string, com fallback para 0.0 ou ''
    """
    try:
        elemento = root.find(f'.//{caminho}')
        
        if elemento is not None and elemento.text:
            texto = elemento.text.strip()
            
            if not texto:
                return 0.0 if '/' in caminho else ''
            
            # Tentar converter para n√∫mero
            try:
                return converter_valor(texto)
            except:
                # Se falhar, retornar como string
                return texto
        
        # Retornar valor padr√£o baseado no tipo esperado
        return 0.0 if '/' in caminho else ''
        
    except Exception as e:
        logger.debug(f"Erro ao buscar '{caminho}': {e}")
        return 0.0 if '/' in caminho else ''


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 4: FUN√á√ïES DE CACHE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def gerar_cache_key(cnpj: str, doc_id: str) -> str:
    """Gera chave √∫nica para cache."""
    return hashlib.md5(f"{cnpj}_{doc_id}".encode()).hexdigest()


def salvar_cache(cnpj: str, doc_id: str, dados: Dict) -> None:
    """Salva resultado em cache."""
    if not CACHE_ENABLED:
        return
    
    try:
        cache_key = gerar_cache_key(cnpj, doc_id)
        cache_file = CACHE_DIR / f"{cache_key}.pkl"
        
        with open(cache_file, 'wb') as f:
            pickle.dump(dados, f)
        
        logger.debug(f"Cache salvo: {cnpj} | doc {doc_id}")
    except Exception as e:
        logger.warning(f"Erro ao salvar cache: {e}")


def ler_cache(cnpj: str, doc_id: str) -> Optional[Dict]:
    """L√™ resultado do cache se existir."""
    if not CACHE_ENABLED:
        return None
    
    try:
        cache_key = gerar_cache_key(cnpj, doc_id)
        cache_file = CACHE_DIR / f"{cache_key}.pkl"
        
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                dados = pickle.load(f)
            logger.debug(f"Cache hit: {cnpj} | doc {doc_id}")
            return dados
        
        return None
    except Exception as e:
        logger.warning(f"Erro ao ler cache: {e}")
        return None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 5: REQUISI√á√ïES COM RETRY LOGIC
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@retry(
    stop=stop_after_attempt(MAX_RETRIES),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((requests.exceptions.RequestException, requests.exceptions.Timeout)),
    reraise=True
)
def fazer_requisicao_com_retry(url: str, **kwargs) -> requests.Response:
    """
    Faz requisi√ß√£o HTTP com retry autom√°tico e backoff exponencial.
    
    Args:
        url: URL para requisi√ß√£o
        **kwargs: Argumentos para requests.get()
        
    Returns:
        Response object
        
    Raises:
        requests.exceptions.RequestException: Ap√≥s MAX_RETRIES tentativas
    """
    logger.debug(f"Requisi√ß√£o: {url[:80]}...")
    response = requests.get(url, **kwargs)
    response.raise_for_status()
    return response


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 6: EXTRA√á√ÉO DE DADOS DO XML (COM BUG FIX)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def extrair_dados_xml_completo(xml_content: bytes) -> Dict[str, Union[str, float, int, None]]:
    """
    Extrai TODOS os campos relevantes do XML de Informe Mensal FIDC.
    
    CORRIGIDO: Bug de convers√£o de tipos no c√°lculo de inadimpl√™ncia.
    
    Args:
        xml_content: Conte√∫do XML em bytes (j√° decodificado do Base64)
        
    Returns:
        Dicion√°rio com 90+ campos estruturados
    """
    try:
        root = ET.fromstring(xml_content)
    except ET.ParseError as e:
        logger.error(f"Erro ao fazer parse do XML: {e}")
        raise
    
    dados = {
        # ‚ïê‚ïê‚ïê SE√á√ÉO 1: IDENTIFICA√á√ÉO DO FUNDO ‚ïê‚ïê‚ïê
        'CNPJ_FUNDO': buscar_valor_xml(root, 'NR_CNPJ_FUNDO'),
        'CNPJ_ADMINISTRADOR': buscar_valor_xml(root, 'NR_CNPJ_ADM'),
        'DATA_COMPETENCIA': buscar_valor_xml(root, 'DT_COMPT'),
        'TIPO_CONDOMINIO': buscar_valor_xml(root, 'TP_CONDOMINIO'),
        'FUNDO_EXCLUSIVO': buscar_valor_xml(root, 'FDO_EXCL'),
        'CLASSE_UNICA': buscar_valor_xml(root, 'CLASS_UNICA'),
        'COTISTA_VINCULADO': buscar_valor_xml(root, 'COTST_VINCUL'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 2: ATIVOS GERAIS ‚ïê‚ïê‚ïê
        'ATIVO_TOTAL': buscar_valor_xml(root, 'VL_SOM_APLIC_ATIVO'),
        'DISPONIBILIDADES': buscar_valor_xml(root, 'VL_DISPONIB'),
        'CARTEIRA_TOTAL': buscar_valor_xml(root, 'VL_CARTEIRA'),
        'OUTROS_ATIVOS_TOTAL': buscar_valor_xml(root, 'VL_SOM_OUTROS_ATIVOS'),
        'OUTROS_ATIVOS_CURTO_PRAZO': buscar_valor_xml(root, 'OUTROS_ATIVOS/VL_OUTRO_VL_RECEB_CURPRZ'),
        'OUTROS_ATIVOS_LONGO_PRAZO': buscar_valor_xml(root, 'OUTROS_ATIVOS/VL_OUTRO_VL_RECEB_LPRAZO'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 3: CR√âDITOS EXISTENTES ‚ïê‚ïê‚ïê
        'CREDITOS_ADQUIRIDOS': buscar_valor_xml(root, 'CRED_EXISTE/VL_SOM_DICRED_AQUIS'),
        'CRED_VENCIDOS_ADIMPLENTES': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_VENC_ADIMPL'),
        'CRED_VENCIDOS_INADIMPLENTES': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_VENC_INAD'),
        'CRED_TOTAL_VENC_INADIMPL': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_TOTAL_VENC_INAD'),
        'CRED_INADIMPLENCIA': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_EXISTE_INAD'),
        'CRED_PERFORMADOS': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_REFER_DICRED_PERFO'),
        'CRED_VENCIDOS_PENDENTES': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_VENC_PEND'),
        'CRED_EMP_RECUPERACAO': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_ORIGEM_EMP_PROC_RECUP'),
        'CRED_RECEITA_PUBLICA': buscar_valor_xml(root, 'CRED_EXISTE/VL_DECOR_RECEIT_PUBLIC'),
        'CRED_ACAO_JUDICIAL': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_ACAO_JUDIC'),
        'CRED_CONSTITUICAO_JURIDICA': buscar_valor_xml(root, 'CRED_EXISTE/VL_CRED_CONST_JUR_FATRISC'),
        'CRED_PROVISAO_REDUCAO': buscar_valor_xml(root, 'CRED_EXISTE/VL_PROVIS_REDUC_RECUP'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 4: DIREITOS CREDIT√ìRIOS (DICRED) ‚ïê‚ïê‚ïê
        'DICRED_TOTAL': buscar_valor_xml(root, 'DICRED/VL_DICRED'),
        'DICRED_CEDENTE': buscar_valor_xml(root, 'DICRED/VL_DICRED_CEDENT'),
        'DICRED_VENC_INADIMPL': buscar_valor_xml(root, 'DICRED/VL_DICRED_EXISTE_VENC_INAD'),
        'DICRED_TOTAL_VENC_INAD': buscar_valor_xml(root, 'DICRED/VL_DICRED_TOTAL_VENC_INAD'),
        'DICRED_INADIMPLENCIA': buscar_valor_xml(root, 'DICRED/VL_DICRED_EXISTE_INAD'),
        'DICRED_PERFORMADOS': buscar_valor_xml(root, 'DICRED/VL_DICRED_REFER_DICRED_PERFO'),
        'DICRED_VENC_PENDENTES': buscar_valor_xml(root, 'DICRED/VL_DICRED_VENC_PEND'),
        'DICRED_EMP_RECUPERACAO': buscar_valor_xml(root, 'DICRED/VL_DICRED_ORIGEM_EMP_PROC_RECUP'),
        'DICRED_RECEITA_PUBLICA': buscar_valor_xml(root, 'DICRED/VL_DICRED_RECEIT_PUBLIC'),
        'DICRED_ACAO_JUDICIAL': buscar_valor_xml(root, 'DICRED/VL_DICRED_ACAO_JUDIC'),
        'DICRED_PROVISAO_REDUCAO': buscar_valor_xml(root, 'DICRED/VL_DICRED_PROVIS_REDUC_RECUP'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 5: VALORES MOBILI√ÅRIOS ‚ïê‚ïê‚ïê
        'VALORES_MOBILIARIOS_TOTAL': buscar_valor_xml(root, 'VALORES_MOB/VL_SOM_VALORES_MOB'),
        'DEBENTURES': buscar_valor_xml(root, 'VALORES_MOB/VL_DEBT'),
        'CRI': buscar_valor_xml(root, 'VALORES_MOB/VL_CRI'),
        'NOTAS_PROMISSORIAS_COMERCIAIS': buscar_valor_xml(root, 'VALORES_MOB/VL_NP_COMERC'),
        'LETRAS_FINANCEIRAS': buscar_valor_xml(root, 'VALORES_MOB/VL_LETRA_FINANC'),
        'COTAS_FIF': buscar_valor_xml(root, 'VALORES_MOB/VL_CLS_COTA_FIF'),
        'OUTROS_DIREITOS_CREDITORIOS': buscar_valor_xml(root, 'VALORES_MOB/VL_OUTRO_DICRED'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 6: OUTROS ATIVOS FINANCEIROS ‚ïê‚ïê‚ïê
        'TITULOS_PUBLICOS_FEDERAIS': buscar_valor_xml(root, 'VL_TITPUB_FED'),
        'CDB': buscar_valor_xml(root, 'VL_CDB'),
        'APLICACOES_COMPROMISSADAS': buscar_valor_xml(root, 'VL_APLIC_OPER_COMPSS'),
        'ATIVOS_FINANCEIROS_RF': buscar_valor_xml(root, 'VL_ATIV_FINANC_RF'),
        'COTAS_FIDC': buscar_valor_xml(root, 'VL_COTA_FIDC'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 7: MERCADO DE DERIVATIVOS ‚ïê‚ïê‚ïê
        'DERIVATIVOS_TOTAL': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_SOM_MERC_DERIVATIVO'),
        'TERMO_COMPRADOR': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_MERC_TERMO_POS_COMPRD'),
        'OPCOES_TITULAR': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_MERC_OP_POS_TITUL'),
        'FUTUROS_AJUSTE_POSITIVO': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_MERC_FUT_AJUST_POSIT'),
        'SWAP_A_RECEBER': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_DIFER_SWAP_RECEB'),
        'COBERTURA_PRESTADA': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_COBERT_PREST'),
        'DEPOSITOS_MARGEM': buscar_valor_xml(root, 'MERC_DERIVATIVO/VL_DEPOS_MARGEM'),
        
        # ‚ïê‚ïê‚ïê SE√á√ÉO 8: SEGMENTA√á√ÉO DA CARTEIRA ‚ïê‚ïê‚ïê
        'CARTEIRA_SEGMENTADA_TOTAL': buscar_valor_xml(root, 'CART_SEGMT/VL_SOM_CART_SEGMT'),
        'SEGMT_INDUSTRIAL': buscar_valor_xml(root, 'CART_SEGMT/VL_IND'),
        'SEGMT_MERCADO_IMOBILIARIO': buscar_valor_xml(root, 'CART_SEGMT/VL_MERC_IMOBIL'),
        'SEGMT_AGRONEGOCIO': buscar_valor_xml(root, 'CART_SEGMT/VL_AGRONEG'),
        'SEGMT_CARTAO_CREDITO': buscar_valor_xml(root, 'CART_SEGMT/VL_CART_CRED'),
        'SEGMT_ACAO_JUDICIAL': buscar_valor_xml(root, 'CART_SEGMT/VL_ACAO_JUDIC'),
        'SEGMT_PROPRIEDADE_INTELECTUAL': buscar_valor_xml(root, 'CART_SEGMT/VL_PROPRD_MARCA_PATENT'),
        
        # Subsegmento: COMERCIAL
        'SEGMT_COMERCIAL_TOTAL': buscar_valor_xml(root, 'SEGMT_COMERC/VL_SOM_SEGMT_COMERC'),
        'SEGMT_COMERCIO': buscar_valor_xml(root, 'SEGMT_COMERC/VL_COMERC'),
        'SEGMT_COMERCIO_VAREJO': buscar_valor_xml(root, 'SEGMT_COMERC/VL_COMERC_VARJ'),
        'SEGMT_ARREND_MERCANTIL': buscar_valor_xml(root, 'SEGMT_COMERC/VL_ARREND_MERCNT'),
        
        # Subsegmento: SERVI√áOS
        'SEGMT_SERVICOS_TOTAL': buscar_valor_xml(root, 'SEGMT_SERV/VL_SOM_SEGMT_SERV'),
        'SEGMT_SERVICOS_GERAIS': buscar_valor_xml(root, 'SEGMT_SERV/VL_SERV'),
        'SEGMT_SERVICOS_PUBLICOS': buscar_valor_xml(root, 'SEGMT_SERV/VL_SERV_PUBLIC'),
        'SEGMT_SERVICOS_EDUCACAO': buscar_valor_xml(root, 'SEGMT_SERV/VL_SERV_EDUC'),
        'SEGMT_SERVICOS_ENTRETENIMENTO': buscar_valor_xml(root, 'SEGMT_SERV/VL_SERV_ENTRETEN'),
        
        # Subsegmento: FINANCEIRO
        'SEGMT_FINANCEIRO_TOTAL': buscar_valor_xml(root, 'SEGMT_FINANC/VL_SOM_SEGMT_FINANC'),
        'SEGMT_FINANC_CREDITO_PESSOA': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_CRED_PESSOA'),
        'SEGMT_FINANC_CONSIGNADO': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_CRED_PESSOA_CONSIG'),
        'SEGMT_FINANC_CORPORATIVO': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_CRED_CORPOR'),
        'SEGMT_FINANC_MIDDLE_MARKET': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_MMARKET'),
        'SEGMT_FINANC_VEICULOS': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_VEICL'),
        'SEGMT_FINANC_IMOB_EMPRESARIAL': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_IMOBIL_EMPSRL'),
        'SEGMT_FINANC_IMOB_RESIDENCIAL': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_IMOBIL_RESID'),
        'SEGMT_FINANC_OUTROS': buscar_valor_xml(root, 'SEGMT_FINANC/VL_FINANC_OUTRO'),
        
        # Subsegmento: FACTORING
        'SEGMT_FACTORING_TOTAL': buscar_valor_xml(root, 'SEGMT_FACT/VL_SOM_SEGMT_FACT'),
        'SEGMT_FACTORING_PESSOA': buscar_valor_xml(root, 'SEGMT_FACT/VL_FACT_PESSOA'),
        'SEGMT_FACTORING_CORPORATIVO': buscar_valor_xml(root, 'SEGMT_FACT/VL_FACT_CORPOR'),
        
        # Subsegmento: SETOR P√öBLICO
        'SEGMT_SETOR_PUBLICO_TOTAL': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SOM_SEGMT_SETOR_PUBLIC'),
        'SEGMT_PRECATORIOS': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SETOR_PUBLIC_PRECAT'),
        'SEGMT_CREDITOS_TRIBUTARIOS': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SETOR_PUBLIC_CRED_TRIBUT'),
        'SEGMT_ROYALTIES': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SETOR_PUBLIC_ROYA'),
        'SEGMT_SETOR_PUBLICO_OUTROS': buscar_valor_xml(root, 'SEGMT_SETOR_PUBLIC/VL_SETOR_PUBLIC_OUTRO'),
    }
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚úÖ BUG FIX: C√ÅLCULO DE INDICADORES COM CONVERS√ÉO FOR√áADA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    # Garantir que valores s√£o float antes de comparar
    inadimpl_cred = float(dados.get('CRED_INADIMPLENCIA', 0) or 0)
    inadimpl_dicred = float(dados.get('DICRED_INADIMPLENCIA', 0) or 0)
    dados['INADIMPLENCIA_TOTAL'] = max(inadimpl_cred, inadimpl_dicred)
    
    carteira_credito = float(dados.get('CREDITOS_ADQUIRIDOS', 0) or 0)
    ativo_total = float(dados.get('ATIVO_TOTAL', 0) or 0)
    disponibilidades = float(dados.get('DISPONIBILIDADES', 0) or 0)
    
    # √çndice de NPL
    if carteira_credito > 0 and dados['INADIMPLENCIA_TOTAL'] > 0:
        dados['INDICE_NPL_PERCENTUAL'] = (dados['INADIMPLENCIA_TOTAL'] / carteira_credito) * 100
    else:
        dados['INDICE_NPL_PERCENTUAL'] = 0.0
    
    # Taxa de liquidez
    if ativo_total > 0:
        dados['TAXA_LIQUIDEZ_PERCENTUAL'] = (disponibilidades / ativo_total) * 100
    else:
        dados['TAXA_LIQUIDEZ_PERCENTUAL'] = 0.0
    
    # Concentra√ß√£o em cr√©dito
    if ativo_total > 0:
        dados['CONCENTRACAO_CREDITO_PERCENTUAL'] = (carteira_credito / ativo_total) * 100
    else:
        dados['CONCENTRACAO_CREDITO_PERCENTUAL'] = 0.0
    
    # Valida√ß√£o de NPL outlier
    if dados['INDICE_NPL_PERCENTUAL'] > 1000:
        logger.warning(f"NPL outlier detectado: {dados['INDICE_NPL_PERCENTUAL']:.2f}% | CNPJ: {dados.get('CNPJ_FUNDO', 'N/A')}")
        dados['NPL_OUTLIER_FLAG'] = True
    else:
        dados['NPL_OUTLIER_FLAG'] = False
    
    return dados


logger.info("‚úÖ Fun√ß√£o de extra√ß√£o definida (90+ campos + bug fix)")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 7: FUN√á√ÉO ETL PRINCIPAL COM PARALELIZA√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def etl_fidc_historico(
    cnpj_alvo: str, 
    nome_fundo_referencia: str = "", 
    anos_dezembro: List[int] = None, 
    coletar_ultimo: bool = True
) -> List[Dict]:
    """
    Executa ETL completo para um CNPJ com cache e retry.
    
    Args:
        cnpj_alvo: CNPJ do fundo (14 d√≠gitos)
        nome_fundo_referencia: Nome do fundo (opcional)
        anos_dezembro: Anos para buscar dezembro
        coletar_ultimo: Se True, coleta √∫ltimo m√™s
        
    Returns:
        Lista de dicion√°rios com dados extra√≠dos
    """
    
    resultados_fundo_cnpj = []
    
    if anos_dezembro is None:
        anos_dezembro = ANOS_DEZEMBRO
    
    try:
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ETAPA 1: DISCOVERY (COM RETRY)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        
        params = {
            'd': 0,
            's': 0,
            'l': 200,
            'cnpjFundo': cnpj_alvo
        }
        
        resp_busca = fazer_requisicao_com_retry(
            URL_API_BUSCA,
            params=params,
            headers=HEADERS,
            timeout=TIMEOUT_BUSCA
        )
        
        data = resp_busca.json().get('data', [])
        
        if not data:
            logger.debug(f"CNPJ {cnpj_alvo}: Nenhum documento encontrado")
            return []
        
        df_docs = pd.DataFrame(data)
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ETAPA 2: FILTER
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        
        df_mensal = df_docs[
            (df_docs['tipoDocumento'].str.strip() == "Informe Mensal Estruturado") &
            (df_docs['situacaoDocumento'].str.strip() == "A")
        ].copy()
        
        if df_mensal.empty:
            logger.debug(f"CNPJ {cnpj_alvo}: Nenhum informe mensal ativo")
            return []
        
        # Converter datas
        df_mensal['dataReferenciaOrdenavel'] = pd.to_datetime(
            df_mensal['dataReferencia'].str[3:] + '-' + 
            df_mensal['dataReferencia'].str[:2] + '-01',
            format='%Y-%m-%d', errors='coerce'
        )
        
        df_mensal.dropna(subset=['dataReferenciaOrdenavel'], inplace=True)
        
        if df_mensal.empty:
            logger.debug(f"CNPJ {cnpj_alvo}: Datas inv√°lidas")
            return []
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ETAPA 3: SELE√á√ÉO DE DOCUMENTOS
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        
        documentos_processar = []
        
        # √öltimo m√™s
        if coletar_ultimo:
            df_mensal_sorted = df_mensal.sort_values('dataReferenciaOrdenavel', ascending=False)
            df_ultimo = df_mensal_sorted.groupby('descricaoFundo').first().reset_index()
            documentos_processar.extend(df_ultimo.to_dict('records'))
        
        # Dezembros
        for ano in anos_dezembro:
            data_dezembro = f"12/{ano}"
            docs_dezembro = df_mensal[df_mensal['dataReferencia'] == data_dezembro]
            
            if not docs_dezembro.empty:
                df_dez_unico = docs_dezembro.groupby('descricaoFundo').first().reset_index()
                documentos_processar.extend(df_dez_unico.to_dict('records'))
        
        # Remover duplicatas por ID
        ids_processados = set()
        docs_unicos = []
        for doc in documentos_processar:
            doc_id = doc['id']
            if doc_id not in ids_processados:
                docs_unicos.append(doc)
                ids_processados.add(doc_id)
        
        logger.debug(f"CNPJ {cnpj_alvo}: {len(docs_unicos)} documentos para processar")
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ETAPA 4-6: DOWNLOAD, EXTRACT, CLEANUP (COM CACHE)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        
        for doc_info in docs_unicos:
            doc_id = str(doc_info['id'])
            data_referencia = doc_info.get('dataReferencia', 'N/A')
            nome_fundo_doc = doc_info.get('descricaoFundo', 'Nome N/A')
            
            # Tentar ler do cache primeiro
            dados_cache = ler_cache(cnpj_alvo, doc_id)
            if dados_cache is not None:
                resultados_fundo_cnpj.append(dados_cache)
                continue
            
            nome_arquivo_temp = None
            
            try:
                # Download com retry
                url_download = f"{URL_API_DOWNLOAD}?id={doc_id}"
                resp_download = fazer_requisicao_com_retry(
                    url_download,
                    headers=HEADERS,
                    timeout=TIMEOUT_DOWNLOAD
                )
                
                xml_content = base64.b64decode(resp_download.content)
                
                # Salvar temporariamente
                nome_arquivo_temp = f'temp_{cnpj_alvo}_{doc_id}.xml'
                with open(nome_arquivo_temp, 'wb') as f:
                    f.write(xml_content)
                
                # Extrair dados
                dados_extraidos = extrair_dados_xml_completo(xml_content)
                
                if not dados_extraidos.get('CNPJ_FUNDO'):
                    dados_extraidos['CNPJ_FUNDO'] = cnpj_alvo
                
                # Metadados
                dados_extraidos['NOME_FUNDO'] = nome_fundo_doc
                dados_extraidos['STATUS'] = 'SUCESSO'
                dados_extraidos['ID_DOCUMENTO'] = doc_id
                dados_extraidos['DATA_REFERENCIA_DOC'] = data_referencia
                dados_extraidos['MENSAGEM_ERRO'] = None
                
                # Tipo de coleta
                if data_referencia.startswith('12/'):
                    ano = data_referencia.split('/')[1]
                    dados_extraidos['TIPO_COLETA'] = f'DEZEMBRO_{ano}'
                else:
                    dados_extraidos['TIPO_COLETA'] = 'ULTIMO_MES'
                
                # Salvar em cache
                salvar_cache(cnpj_alvo, doc_id, dados_extraidos)
                
                resultados_fundo_cnpj.append(dados_extraidos)
                
            except requests.exceptions.HTTPError as e:
                logger.warning(f"HTTP Error | CNPJ {cnpj_alvo} | Doc {doc_id}: {e}")
                resultados_fundo_cnpj.append({
                    'CNPJ_FUNDO': cnpj_alvo,
                    'NOME_FUNDO': nome_fundo_doc,
                    'STATUS': 'ERRO_HTTP_DOWNLOAD',
                    'MENSAGEM_ERRO': f'HTTP Error: {str(e)}',
                    'DATA_REFERENCIA_DOC': data_referencia,
                    'TIPO_COLETA': 'ERRO'
                })
            except requests.exceptions.Timeout as e:
                logger.warning(f"Timeout | CNPJ {cnpj_alvo} | Doc {doc_id}: {e}")
                resultados_fundo_cnpj.append({
                    'CNPJ_FUNDO': cnpj_alvo,
                    'NOME_FUNDO': nome_fundo_doc,
                    'STATUS': 'TIMEOUT_DOWNLOAD',
                    'MENSAGEM_ERRO': f'Timeout: {str(e)}',
                    'DATA_REFERENCIA_DOC': data_referencia,
                    'TIPO_COLETA': 'ERRO'
                })
            except ET.ParseError as e:
                logger.error(f"Parse Error | CNPJ {cnpj_alvo} | Doc {doc_id}: {e}")
                resultados_fundo_cnpj.append({
                    'CNPJ_FUNDO': cnpj_alvo,
                    'NOME_FUNDO': nome_fundo_doc,
                    'STATUS': 'ERRO_PARSE_XML',
                    'MENSAGEM_ERRO': f'Erro XML: {str(e)}',
                    'DATA_REFERENCIA_DOC': data_referencia,
                    'TIPO_COLETA': 'ERRO'
                })
            except Exception as e:
                logger.error(f"Erro inesperado | CNPJ {cnpj_alvo} | Doc {doc_id}: {e}", exc_info=True)
                resultados_fundo_cnpj.append({
                    'CNPJ_FUNDO': cnpj_alvo,
                    'NOME_FUNDO': nome_fundo_doc,
                    'STATUS': 'ERRO_INESPERADO',
                    'MENSAGEM_ERRO': f'Erro: {str(e)}',
                    'DATA_REFERENCIA_DOC': data_referencia,
                    'TIPO_COLETA': 'ERRO'
                })
            
            finally:
                # Cleanup
                if nome_arquivo_temp and os.path.exists(nome_arquivo_temp):
                    try:
                        os.remove(nome_arquivo_temp)
                    except OSError:
                        pass
    
    except Exception as e:
        logger.error(f"Erro geral | CNPJ {cnpj_alvo}: {e}", exc_info=True)
        resultados_fundo_cnpj.append({
            'CNPJ_FUNDO': cnpj_alvo,
            'NOME_FUNDO': nome_fundo_referencia,
            'STATUS': 'ERRO_GERAL',
            'MENSAGEM_ERRO': f'Erro geral: {str(e)}',
            'TIPO_COLETA': 'ERRO'
        })
    
    return resultados_fundo_cnpj


logger.info("‚úÖ Fun√ß√£o ETL com retry, cache e paraleliza√ß√£o pronta")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 8: CARREGAMENTO DOS CNPJs
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

ARQUIVO_CSV_ENTRADA = 'lista_cnpjs_fidc.csv'

try:
    df_cnpjs_entrada = pd.read_csv(ARQUIVO_CSV_ENTRADA, encoding='utf-8-sig')
    
    logger.info(f"‚úÖ Arquivo '{ARQUIVO_CSV_ENTRADA}' carregado")
    logger.info(f"   Total de CNPJs: {len(df_cnpjs_entrada)}")
    
    df_cnpjs_entrada['CNPJ'] = df_cnpjs_entrada['CNPJ'].astype(str).str.replace('.0', '', regex=False)
    df_cnpjs_entrada['CNPJ'] = df_cnpjs_entrada['CNPJ'].str.zfill(14)
    
    # Estimativa de tempo
    docs_estimados_por_cnpj = (1 if COLETAR_ULTIMO_MES else 0) + len(ANOS_DEZEMBRO)
    tempo_estimado_seq = len(df_cnpjs_entrada) * DELAY_ENTRE_REQUISICOES * docs_estimados_por_cnpj / 60
    tempo_estimado_par = tempo_estimado_seq / MAX_WORKERS
    
    logger.info(f"‚è±Ô∏è  Tempo estimado:")
    logger.info(f"   Sequencial: ~{tempo_estimado_seq:.1f} min")
    logger.info(f"   Paralelo ({MAX_WORKERS} threads): ~{tempo_estimado_par:.1f} min")
    logger.info(f"   Ganho: {(1 - tempo_estimado_par/tempo_estimado_seq)*100:.0f}%")

except FileNotFoundError:
    logger.error(f"‚ùå Arquivo '{ARQUIVO_CSV_ENTRADA}' n√£o encontrado!")
    raise

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 9: PROCESSAMENTO EM LOTE COM PARALELIZA√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

logger.info("="*80)
logger.info("üöÄ INICIANDO PROCESSAMENTO PARALELO")
logger.info("="*80)

resultados_consolidados = []
tempo_inicio = datetime.now()

# Processar com ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    # Submeter todas as tarefas
    future_to_cnpj = {}
    for idx, row in df_cnpjs_entrada.iterrows():
        cnpj = row['CNPJ']
        nome_fundo_ref = row.get('NOME_FUNDO', '')
        
        future = executor.submit(
            etl_fidc_historico,
            cnpj,
            nome_fundo_ref,
            ANOS_DEZEMBRO,
            COLETAR_ULTIMO_MES
        )
        future_to_cnpj[future] = (cnpj, nome_fundo_ref)
    
    # Processar conforme completam
    with tqdm(total=len(df_cnpjs_entrada), desc="Processando CNPJs", unit="cnpj") as pbar:
        for future in as_completed(future_to_cnpj):
            cnpj, nome_fundo_ref = future_to_cnpj[future]
            
            try:
                resultados_atuais = future.result()
                
                if resultados_atuais:
                    resultados_consolidados.extend(resultados_atuais)
                    
                    # Log sucesso
                    sucessos = [r for r in resultados_atuais if r.get('STATUS') == 'SUCESSO']
                    if sucessos:
                        num_docs = len(sucessos)
                        pbar.set_postfix({'√∫ltimos_docs': num_docs, 'status': '‚úÖ'})
                    else:
                        pbar.set_postfix({'status': '‚ùå Erro'})
                else:
                    resultados_consolidados.append({
                        'CNPJ_FUNDO': cnpj,
                        'NOME_FUNDO': nome_fundo_ref,
                        'STATUS': 'NENHUM_DOCUMENTO',
                        'TIPO_COLETA': 'ERRO'
                    })
                    pbar.set_postfix({'status': '‚ùå Sem docs'})
                
            except Exception as e:
                logger.error(f"Erro ao processar futuro de {cnpj}: {e}")
                pbar.set_postfix({'status': '‚ùå Exception'})
            
            pbar.update(1)
            
            # Rate limiting suave entre completions
            time.sleep(0.1)

tempo_decorrido = (datetime.now() - tempo_inicio).total_seconds()

logger.info("="*80)
logger.info("‚úÖ PROCESSAMENTO CONCLU√çDO!")
logger.info(f"   Tempo decorrido: {tempo_decorrido/60:.2f} minutos")
logger.info("="*80)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 10: CONSOLIDA√á√ÉO E ESTAT√çSTICAS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

df_resultado_final = pd.DataFrame(resultados_consolidados)

total_documentos = len(df_resultado_final)
total_sucesso = len(df_resultado_final[df_resultado_final['STATUS'] == 'SUCESSO'])
total_erros = total_documentos - total_sucesso

logger.info("üìä ESTAT√çSTICAS DO PROCESSAMENTO")
logger.info("-"*80)
logger.info(f"   Total de CNPJs processados: {len(df_cnpjs_entrada)}")
logger.info(f"   Total de documentos coletados: {total_documentos}")
logger.info(f"   ‚úÖ Sucesso: {total_sucesso} ({total_sucesso/total_documentos*100:.1f}%)")
logger.info(f"   ‚ùå Erros: {total_erros} ({total_erros/total_documentos*100:.1f}%)")

# Distribui√ß√£o por tipo
if total_sucesso > 0:
    df_sucesso = df_resultado_final[df_resultado_final['STATUS'] == 'SUCESSO'].copy()
    
    logger.info("\nüìÖ DISTRIBUI√á√ÉO POR TIPO DE COLETA:")
    tipos_coleta = df_sucesso['TIPO_COLETA'].value_counts()
    for tipo, count in tipos_coleta.items():
        logger.info(f"   ‚Ä¢ {tipo}: {count} documentos")

# Distribui√ß√£o de erros
if total_erros > 0:
    logger.info("\n‚ö†Ô∏è  TIPOS DE ERROS:")
    erros_por_tipo = df_resultado_final[df_resultado_final['STATUS'] != 'SUCESSO']['STATUS'].value_counts()
    for status, count in erros_por_tipo.items():
        logger.info(f"   ‚Ä¢ {status}: {count}")

# Estat√≠sticas financeiras
if total_sucesso > 0:
    for col in ['ATIVO_TOTAL', 'INADIMPLENCIA_TOTAL', 'INDICE_NPL_PERCENTUAL']:
        df_sucesso[col] = pd.to_numeric(df_sucesso[col], errors='coerce')
    
    df_sucesso_valido = df_sucesso[df_sucesso['ATIVO_TOTAL'] > 0]
    
    if len(df_sucesso_valido) > 0:
        logger.info("\nüí∞ ESTAT√çSTICAS FINANCEIRAS")
        logger.info("-"*80)
        
        ativo_total_soma = df_sucesso_valido['ATIVO_TOTAL'].sum()
        ativo_total_media = df_sucesso_valido['ATIVO_TOTAL'].mean()
        inadimpl_soma = df_sucesso_valido['INADIMPLENCIA_TOTAL'].sum()
        npl_medio = df_sucesso_valido['INDICE_NPL_PERCENTUAL'].mean()
        
        logger.info(f"   Ativo Total (Soma):     R$ {ativo_total_soma:,.2f}")
        logger.info(f"   Ativo Total (M√©dia):    R$ {ativo_total_media:,.2f}")
        logger.info(f"   Inadimpl√™ncia (Soma):   R$ {inadimpl_soma:,.2f}")
        logger.info(f"   NPL M√©dio:              {npl_medio:.2f}%")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√âLULA 11: EXPORTA√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# CSV principal
arquivo_csv = 'base_fidc_completa_historica_v5.csv'
df_resultado_final.to_csv(arquivo_csv, index=False, encoding='utf-8-sig', sep=';', decimal=',')
logger.info(f"\nüíæ Arquivo salvo: {arquivo_csv}")

# Excel com m√∫ltiplas abas
arquivo_excel = 'base_fidc_completa_historica_v5.xlsx'

try:
    with pd.ExcelWriter(arquivo_excel, engine='openpyxl') as writer:
        df_resultado_final.to_excel(writer, sheet_name='Dados_Completos', index=False)
        
        if total_sucesso > 0:
            df_sucesso.to_excel(writer, sheet_name='Dados_Validos', index=False)
        
        if total_erros > 0:
            df_erros = df_resultado_final[df_resultado_final['STATUS'] != 'SUCESSO']
            df_erros.to_excel(writer, sheet_name='Erros', index=False)
        
        if COLETAR_ULTIMO_MES and total_sucesso > 0:
            df_ultimo = df_sucesso[df_sucesso['TIPO_COLETA'] == 'ULTIMO_MES']
            if not df_ultimo.empty:
                df_ultimo.to_excel(writer, sheet_name='Ultimo_Mes', index=False)
        
        for ano in ANOS_DEZEMBRO:
            tipo_coleta = f'DEZEMBRO_{ano}'
            df_ano = df_sucesso[df_sucesso['TIPO_COLETA'] == tipo_coleta]
            if not df_ano.empty:
                df_ano.to_excel(writer, sheet_name=f'Dezembro_{ano}', index=False)
    
    logger.info(f"üíæ Arquivo salvo: {arquivo_excel}")
except Exception as e:
    logger.warning(f"N√£o foi poss√≠vel gerar Excel: {e}")

# Log final
logger.info("\n" + "="*80)
logger.info("‚úÖ ETL FIDC V5 CONCLU√çDO COM SUCESSO!")
logger.info("="*80)
logger.info(f"üìÅ Arquivos gerados:")
logger.info(f"   ‚Ä¢ {arquivo_csv}")
logger.info(f"   ‚Ä¢ {arquivo_excel}")
logger.info(f"   ‚Ä¢ {log_filename}")
logger.info(f"\nüéØ Melhorias implementadas:")
logger.info(f"   1. ‚úÖ Bug fix - Convers√£o de tipos corrigida")
logger.info(f"   2. ‚úÖ Logging profissional ({log_filename})")
logger.info(f"   3. ‚úÖ Retry com backoff exponencial (at√© {MAX_RETRIES}x)")
logger.info(f"   4. ‚úÖ Paraleliza√ß√£o ({MAX_WORKERS} threads)")
logger.info(f"   5. ‚úÖ Cache de resultados ({CACHE_DIR}/)")
logger.info(f"   6. ‚úÖ Progress bar visual")
logger.info(f"   7. ‚úÖ Valida√ß√£o de NPL outliers")
logger.info("\n‚è±Ô∏è  Performance:")
logger.info(f"   Tempo de execu√ß√£o: {tempo_decorrido/60:.2f} minutos")
logger.info(f"   M√©dia por CNPJ: {tempo_decorrido/len(df_cnpjs_entrada):.2f} segundos")
logger.info(f"   Taxa de sucesso: {total_sucesso/total_documentos*100:.1f}%")
logger.info("="*80)
